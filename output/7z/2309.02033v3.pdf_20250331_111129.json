{
  "original_file": "d:\\project\\data_clean\\input\\222.7z\\2309.02033v3.pdf",
  "processed_time": "20250331_111129",
  "documents": [
    {
      "content": "Data-Juicer: A One-Stop Data Processing System for Large\nLanguage Models\nDaoyuan Chen∗, Yilun Huang∗, Zhijian Ma∗, Hesen Chen∗, Xuchen Pan†, Ce Ge†, Dawei Gao†,\nYuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li‡, Bolin Ding‡, Jingren Zhou\nAlibaba Group\nABSTRACT\nThe immense evolution in Large Language Models (LLMs) has\nunderscored the importance of massive, heterogeneous, and high-\nquality data. A data recipe is a mixture of data of different types\nand from different sources for training an LLM, which has been\nknown as one of the most important factors that decide the LLM’s\nperformance. Existing open-source tools for LLM data processing\nare mostly tailored for preparing specific data recipes. To continu-\nously uncover the potential of LLMs, incorporate (after cleaning)\ndata from new sources, and improve LLMs’ general-purpose or\ndomain-specific performance, we build a data processing system,\nnamed Data-Juicer, with which we can efficiently generate di-\nverse data recipes, explore different possibilities in forming the data\nmixtures, and evaluate their effects on the model performance. Dif-\nferent from traditional data-analytics pipelines, Data-Juicer faces\nsome unique challenges. Firstly, the possible data sources for form-\ning data recipes are truly heterogeneous and massive with various\nqualities (e.g., considering all web-pages on the Internet). Secondly,\nit is extremely expensive to precisely evaluate data recipes’ impact\non the LLMs’ performance. Thirdly, sufficient flexibility needs to\nbe provided to the end users of Data-Juicer, model developers, to\nconfigure and evaluate different data recipes.\nData-Juicer features a fine-grained abstraction of the pipeline\nfor constructing data recipes, with over 50 built-in operators that\ncan be freely composed and extended. By incorporating visualiza-\ntion and auto-evaluation capabilities,Data-Juicer enables a timely\nfeedback loop after data processing for both LLM pre-training and\nfine-tuning. Further,Data-Juicer is optimized and integrated with\necosystems for LLM training, evaluation, and distributed comput-\ning. With the help of Data-Juicer, we derive data recipes that\nachieve remarkable performance boosts on state-of-the-art LLMs,\ndemonstrating up to 7.45% increase in averaged score across 16\nLLM benchmarks and 17.5% higher win rate in pair-wise GPT-4\nevaluations. More importantly, we hope that Data-Juicer pro-\nmotes broader data-centric research on training and understanding\nLLMs. Data-Juicer and our data recipes are released and actively\nmaintained at https://github.com/alibaba/data-juicer.\n1 INTRODUCTION\nLarge Language Models (LLMs) [9, 18, 69, 70, 90, 92] have achieved\nunprecedented intelligence, enabling applications that would other-\nwise be infeasible due to unsatisfied performance. As the “food” for\nLLMs, data plays a pivotal role in these exciting advancements\n[31, 62, 71, 103]. LLMs are built by pre-training on large-scale\n∗Co-first authors.\n†Equal contribution.\n‡Corresponding authors, email addresses: {yaliang.li, bolin.ding}@alibaba-inc.com\ngeneral-purpose corpus and are fine-tuned with specific-purpose\ndata for alignment or downstream tasks. For pre-training data, a\ncollection of diverse data, including web texts, dialogues, academic\npapers, code bases, and others, help to develop the vast repository\nof knowledge and great applicability [9, 57, 75]. Fine-tuning data,\nwhich further refines LLMs and aligns model behavior with human\nvalues [3, 48, 68]. As “garbage in, garbage out” suggests, the input\ndata for training or tuning an LLM has a direct impact on the quality\nof the derived model [ 35, 44]. Building effective data processing\nsolutions for LLMs remains a sophisticated yet fully under-explored\ntask, given the common challenges in processing both pre-training\nand fine-tuning data, which pursue good data quality, proper data\ndiversity, and large data volume.\nUnfortunately, there exist only a few open-source projects con-\ntributing their LLM training data and the corresponding processing\ncodes [24, 51], particularly in comparison to numerous open-source\nprojects on models and training infrastructures [6, 7, 19, 67, 80, 93,\n105]. Such limited development of data processing will obstruct the\nprogress of quantitatively understanding and enhancing LLMs from\nthe perspective of data, especially accompanied by the following\nnoteworthy Challenges for LLM data processing.\n(C1) High Heterogeneity in LLM’s Data Recipe. LLMs in-\nvolve several developmental stages and enable diverse usages in-\ncluding coding and dialog assistance, and even aiming at Artificial\nGeneral Intelligence. As a result, they demand an extensive variety\nof data types, formats, and quality in their training data, leading\nto highly complex data-processing pipelines. A data recipe for\ntraining or tuning an LLM is such a mixture of processed data from\ndifferent types of sources, with their ratios and processing pipelines\nproperly set [24, 25]. Existing systems, e.g., [24, 80], release certain\nprocessing scripts to generate data recipes for the pre-training pur-\npose, whereas [17, 92] focus on data recipes for improving data\ndiversity and quality in LLaMA’s [93] fine-tuning stage. However,\ndue to the lack of abstraction of processing pipelines and compos-\nability of operators (OPs), such as those for data editing, cleaning,\nand filtering, it is difficult to incorporate new data sources in data\nrecipes provided by these systems, or to extend their pipelines for\nexploring other possibilities of data recipes.\n(C2) Timely Feedback for Data Recipe. The search space of\nLLM’s data recipes is huge due to the high degree of heterogeneity\nin data sources and numerous ways to mix them (with proper pro-\ncessing OPs, combinations, and ratios). We want to explore as many\ndata recipes in the search space as possible with timely feedback\nto uncover the potential of LLMs and improve their performance.\nHowever, as the size of an LLM (number of model parameters) is\nusually billions or even larger, it is super expensive, in terms of\nboth the time and computational resources, to evaluate the impact\narXiv:2309.02033v3  [cs.LG]  20 Dec 2023",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 0
      }
    },
    {
      "content": "Zero-code Data Processing\n data cleandata mixturedata re-format\nOff-the-shelf Data Processing Components Quality Classifiers(GPT-3, chinese, code, ...)Analyzers(OP-effect, HPO, ...)Formatters(unify json, txt, pdf, ...)\nMappers(transform data in-place)Filters(remove specific info) Reference LMs(LLaMA, ModelScope, ...)Visualizers(histgram, diversity, ...)\nFlexible & Well-documented Configuration \nVersatile & Resuable OPsDedicated & Pluggable Tools\nfor Pre-training(RedPajama, oscar, refined, ...) for Fine-tuning (instruction, alignment, refined, ...)\nPlentiful Data Recipes & Demos \nLow-code Customization\n(Experienced Users)\nPre-training/Fine-Tuning(Megatron-LM, Transformers, ...) Auto-Evaluation(LLM API, HELM, ...)Feedback\nDeduplicators(compare in multile views)\nLLM Ecosystems\nDistributed ComputingEcosytems\n......\ndata probe\nSampler(meta, stats, ...)Tracer(lineage, ...)\nCheckpoints\nOP  Fusion (context, reordering)\n(Novice Users)\n(Take-it-away Users)\nFigure 1: Overview of Data-Juicer.\nof a data recipe on the LLM’s performance by training or tuning it\nwith the recipe [85] and running evaluation benchmarks [59].\n(C3) Usability and Customizability. The workflow of training\nor tuning LLMs starts from processing raw data. Exacerbated by\nthe above two challenges, there is an urgent need for a data-centric\ninfrastructure, so that the model developers can easily re-use or\nimplement their own OPs and tools for data processing, configure\ntheir processing pipeline, explore various data recipes, and eval-\nuate the resulting LLMs’ performance. We need such a system to\naccelerate the exploration and understanding of LLMs’ potentials.\n(C4) Massive Data Volume. Last but not least, LLMs are trained\non vast corpora, with data volumes stretching to an unprecedented\nmagnitude of billions or even trillions of tokens (a modeling unit\nof text dependent on the used tokenizer [49]). Efficient LLM data\nprocessing of such volume is critical but arduous. However, consid-\nerations on system performance optimization are often bypassed\nby existing studies, leaving significant room for enhancement in en-\nsuring the stability of data processing and facilitating the deliveries\nof processed data and trained weights for LLMs.\nOverview ofData-Juicer. In this paper, we advocate for a one-\nstop data processing system that addresses these challenges, en-\nabling comprehensive, user-friendly, and efficient data processing\nabilities to facilitate data-centric LLM research and development.\nThe proposed system, named Data-Juicer and illustrated in a\nbottom-up view in Figure 1, is strategically designed to generate\ndata recipes making data more “juicy” and digestible for LLMs.\nWe decouple the mixture elements of existing solutions for LLM\ndata processing, such as specific data types, auxiliary models, and\ndownstream tasks. As highlighted by the green boxes,Data-Juicer\nfosters a fine-grained abstraction and implementation of compos-\nable modules with over 50 versatile OPs and dedicated tools. We\nmake Data-Juicer end-to-end configurable to help prepare trace-\nable, comparable, and refinable data recipes at various scenarios\nof LLM pre-training and fine-tuning, as shown in the yellow and\npink boxes. Coupled with established auto-evaluation capabilities,\nData-Juicer supports a timely feedback loop at multiple devel-\nopment stages of data recipes and LLMs, thereby promoting the\nproduction of valuable LLM data.\nTo meet diverse user backgrounds and needs (marked by the\nleft three rectangle boxes), we design Data-Juicer as an easy-to-\nuse, flexible and extensible system. Beginners are shielded from\nunderlying complexities and benefit from numerous ready-to-use\ndatasets, data recipes, and pluggable tools, supporting zero-code\nLLM data processing. With the help of the flexible configuration\nmodule, experienced users can simply modify built-in data recipes,\nreorganize the order of OPs and tools, and tune the value of their\nhyper-parameters, to meet their lightweight customization needs.\nThanks to the standardization and modularization, advanced users\nare empowered to conveniently extend and register their new OPs\nand tools into Data-Juicer, facilitating quick engagement in sec-\nondary development. Furthermore, we offer more than a dozen\ninteractive tutorials implemented by streamlit [87] to help users\nwith their LLM data processing journey.\nData-Juicer hinges itself on the Huggingface-datasets library\n[55], providing a unified intermediate representation of data and\nachieving optimized space-time efficiency and robustness through\nvarious techniques such as context management, OP fusion, caching,\nand checkpoint mechanisms. Furthermore, as the right two circles\nshow, Data-Juicer seamlessly integrates with ecosystems for LLM\ntraining and evaluation such as Megatron-LM [85] and HELM [59],\nand distributed computing ecosystems such as Ray [66] and Beam\n[5], thus facilitating comprehensive LLM data processing and en-\nhancing large-scale data processing capabilities.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 1
      }
    },
    {
      "content": "Leveraging the proposed system, we refine several open-sourced\ndatasets and derive numerous data recipes for both LLM pre-trained\nand fine-tuning. These refined datasets are not only higher in qual-\nity but also more digestible by LLMs, leading to effective perfor-\nmance improvements of LLMs. Empirical analysis showcases an\nimprovement of up to 7.45% averaged score across 16 LLM bench-\nmarks using our refined pre-training data. Even pre-trained on only\n43% quantity of compared data, we observe superior performance\nover state-of-the-art (SOTA) LLMs such as Falcon [1]. Moreover,\ncompared with SOTA LLMs fine-tuned on competitive open English\nand Chinese data, LLMs fine-tuned on Data-Juicer’s data gain an\naverage of 10.3% higher win rate of pair-wise GPT-4 evaluation,\nwhile with an average 56.8% fewer data quantity. Finally, we intro-\nduce its utility in real-world deployment, and validate its superior\nsystem efficiency and scalability of Data-Juicer, by up to 88.7%\nreduction in single-machine processing time and 77.1% savings in\nmemory usage, and 7.91x distributed processing acceleration.\nContributions. Our contributions are summarized as follows:\n•We propose and build a novel system for LLM data processing,\nData-Juicer, which is featured by decoupled modules and over\n50 versatile OPs and tools. To easily dive into data quality and\ninsights, Data-Juicer fosters a timely feedback loop with inter-\nactive visualizations and auto-evaluation capabilities.\n• Demonstrated by extensive empirical evidence, Data-Juicer\nproduces numerous high-quality data recipes to enhance LLMs\nand exhibits superior system performance, powered by dedicated\noptimization and integrated distributed computing ecosystems.\n•We integrate data-centric methodologies for LLM data processing\nand LLM development with user-centric interface designs, with\nthe aim that Data-Juicer can ease access for diverse users and\ndemocratize LLM data processing.\n• To promote further research and development, our system, data\nrecipes, and tutorials are maintained and released at https://\ngithub.com/alibaba/data-juicer, which we hope can help pave\nthe way for next-generation production paradigms of LLM data.\nOrganization. The subsequent sections describe Data-Juicer in\ndetail. Sec. 2 elaborates on the background and related studies. Sec.\n3 outlines our OP pool, as a response to high heterogeneity of\nLLM data recipes (C1). Sec. 4 delves into our formulation of timely\nfeedback loops for data processing and development of LLMs (C2).\nSec. 5 details our repository of data recipes and tools that counteract\nusability and customization issues ( C3). Sec. 6 expounds on the\nemployed system optimization to tackle massive data volume (C4).\nSec. 7 focuses on an extensive empirical evaluation for the quality\nof data recipes, performance and usability of Data-Juicer. Lastly,\nwe draw a summary in Sec. 8.\n2 BACKGROUND AND RELATED WORKS\n2.1 Large Language Model (LLM) Data\nLarge Language Models (LLMs). Language modeling is a crucial\ncomponent for achieving machine intelligence [ 65, 109]. In the\nlast few years, this field has witnessed remarkable advancements,\nparticularly with the emergence of the pre-training and fine-tuning\nparadigms, where language models undergo an initial phase of\ntraining with a general-purpose corpus before being fine-tuned\nwith specific-purpose tasks [ 27, 72]. This procedure has yielded\nexceptional performance across a spectrum of natural language\nprocessing (NLP) tasks [54, 76].\nRecently, taking advantage of the highly parallelizable nature of\nthe self-supervised Transformer architecture, the scales of model\nparameters and training corpus for LLMs have significantly been\nincreased [28, 69]. Meanwhile, LLMs have aroused considerable\ninterest in the potential of artificial general intelligence [ 10, 11,\n30, 38, 43, 99, 108]. While model-centric studies proliferate, how\nto better process LLM data remains an intricate domain yet to be\ncompletely unfurled, whether for pre-training or fine-tuning data.\nPre-training Data. Pre-training serves as the foundation for\nLLM intelligence. By being trained on large amounts of high-quality\ndata, LLMs can acquire elementary language comprehension and\ngeneration capabilities [37]. Aiming to elucidate the link between\ndata and LLMs intuitively, let us consider a typical pre-training\nobjective prevalent among mainstream LLMs. Given a token se-\nquence [𝑡1,...,𝑡 𝑖,...,𝑡 𝑛], an LLM 𝜃 is trained to maximize the joint\nprobability of the text as follows:\n𝜃0 = arg max\n𝜃\n𝑛∑︁\n𝑖=1\nlog 𝑝(𝑡𝑖 |𝑡1:𝑖−1; 𝜃). (1)\nThis objective is for auto-regressive language modeling and allows\nthe pre-trained 𝜃0 to predict the probability of the next token by\nadhering to the inherent sequential ordering of the language [94].\nExploiting this unified yet simple modeling goal, researchers col-\nlect a large volume and diverse range of corpus data, which usually\ncontains hundreds of billion tokens or even trillion tokens. After\ntokenization and pre-training, LLMs have succeeded in stimulating\na wide range of advanced capabilities. The LLM pre-training data\ngenerally includes various types derived from the web crawlers\n[26, 71], dialogues or social media [107], book-length formal texts\n[36, 110], rigorous encyclopedias and academic texts [31, 100], struc-\ntured coding texts [18, 57], and more texts from financial, medical\nand legal domains [58, 91, 104]. A challenge is nonetheless posed\nin the careful processing and formulation of pre-training data to\nfilter noise, redundancy, irrelevance, and potentially toxic [33, 62].\nFine-tuning Data. Numerous studies have underscored that\nfine-tuning – the process of refining pre-trained LLMs using a\nsmaller, task-specific dataset – can further enhance or unlock addi-\ntional capabilities of LLMs [40, 53, 97, 98]. Crucially, this process\nalso paves the way for better aligning the behavior of these ad-\nvanced models with human values and preferences [60, 68].\nIn this phase, though the data volume decreases exponentially\ncompared to the pre-training phase, the format of fine-tuning data is\nquite different [73]. Typically, given a textual dataset {(𝑥1,𝑠1,𝑦1),...,\n(𝑥𝑗,𝑠𝑗,𝑦𝑗),..., (𝑥𝑚,𝑠𝑚,𝑦𝑚)}, the goal of fine-tuning is to adjust the\npre-trained LLM 𝜃0 to find 𝜃∗that maximizes the likelihood of the\ntask-oriented response 𝑦𝑗 for the user query 𝑥𝑗:\n𝜃∗= arg max\n𝜃\n𝑚∑︁\n𝑗=1\nlog 𝑝(𝑦𝑗 |𝑥𝑗,𝑠𝑗 ; 𝜃); 𝜃 ←𝜃0. (2)\nHere 𝑠𝑗 stands for task-specific instructions, such as “summarize the\nfollowing text: ”, optionally accompanied by a few demonstrative\nsamples for in-context learning [9].\nThe fine-tuning data can be broadly categorized into two types:\nInstruct Fine-Tuning (IFT) datasets to enhance the instruction-following",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 2
      }
    },
    {
      "content": "abilities of LLMs and are usually adapted from existing NLP bench-\nmarks [4, 61]; and Chat Fine-Tuning (CFT) datasets for enhanced\ndialog ability and human value alignment [70, 92]. There are pre-\nliminary explorations emphasizing the importance of data diversity\nover volume for fine-tuning data [20, 95]. Several studies also indi-\ncate that data types representing human values can potentially lead\nto degraded general performance, a phenomenon known as the\n“alignment tax” [70]. However, how to more effectively process the\nfine-tuning data to maximize its usefulness and minimize potential\nrisks remains an open area for further investigation.\nThe Symbiotic Nature of Pre-training and Fine-tuning Data.\nIt is worth pointing out the analogous properties shared between\nthese two types of data, which motivate our synergetic approach\nwhen bearing quality, diversity, and volume considerations in mind.\nSpecifically, the quality aspect of the text has been studied exten-\nsively in existing literature [62]. Efforts have been made to enhance\naspects such as text structure, the soundness of arguments, con-\ntextual richness, writing correctness, comprehensiveness, levels\nof anonymization, and harmlessness. The widespread implemen-\ntation of cleaning, deduplication, and anonymization processes in\npre-training data typifies the aforementioned pursuit. For exam-\nple, researchers may opt to iterate over additional epochs with\nWikipedia-style data in LLM training [93]. Similarly, fine-tuning\ndata processing also employs filtering, deduplication, and detoxifi-\ncation strategies, aiming to enhance the user experience and the\ndegree of aid offered by LLMs [17, 33].\nDiversity is another shared property studied at length in both\ntypes of data. Mixing various types of data and finding suitable mix-\nture weights to achieve appropriate diversity has been a primary\nconcern in works for pre-training data processing [ 103]. Analo-\ngously, efforts for fine-tuning data aim to increase multi-view di-\nversity such as tuning tasks and expression styles, which further\nunderscores this shared property [70, 77, 92].\nIn addition, the pursuit of quality and diversity tends to trade\noff with data volume, which is also reflected in these two types of\ndata. Researchers have incessantly strived to empower LLMs with\nmassive amounts of data, hoping to encapsulate as much human\nknowledge as possible. For instance, there has been an influx in pre-\ntraining data volumes to terabyte levels [69, 71], and fine-tuning\ndata volumes have grown from mere thousands to millions [4, 96].\nHowever, the counter effects of these initiatives are also brought\ninto these large volumes of data, including heightened noise, poten-\ntial inferior quality, and increased bias, which necessitate additional\ndata processing efforts and surging LLM training overheads.\n2.2 Existing LLM Data Processing Solutions\nLLM data processing is an early area that is still working towards\ncommon standards, and we aim to embody a pioneering system\nfor the community. With a commitment to open-source ethos,\nData-Juicer caters to the increasing demand for versatile, flexible,\nuser-friendly and efficientdata processing solutions, details of which\nwill be described later. This contrasts the well-known LLMs that\nwere largely closed-source in data or data processing , such as the\nGPT derivatives [9, 18, 69, 84], LLaMA derivatives [16, 19, 89, 92, 93],\nand others [1, 15, 79, 102, 107]. While some progress has been made\nin the open-source LLM data processing landscape [4, 24, 51, 86],\nthey have not fully delivered the abstraction and breadth of func-\ntionalities that Data-Juicer aims to bring to the forefront of the\nfield.\nExamining this from the perspective of the target datasets, ex-\nisting works typically fixate on specific data sources and use cases\nfor LLMs , spanning alignment of specialized English sub-datasets\nfor LLaMA pre-training [93], assembly of multi-lingual corpora for\npre-training [51], or crowdsourcing for fine-tuning prompt data [4].\nHowever, they lack the systematic and modular processing abilities\nrequired to proficiently manage heterogeneous data, which is an\narea Data-Juicer strives to push its boundaries. These limitations\nbecome especially apparent when handling new data types, engag-\ning in language transfer, or implementing particular data cleaning\nand transformations for LLM applications.\nMoreover, existing works suffer from sub-optimal usability and\nability to explore data insight . Most of these works only offer the\nprocessed data along with purpose-built processing codes specific\nto those data, lacking in ease-of-use considerations and support of\nassistive tool-kits. This hinders their adaptability to diverse users\nand alternative usages. Users might face a daunting task when\nsubstituting data processing goals or conducting analyses due to\na dearth of complementary data-analytical capabilities. The re-\ndevelopment of data processing tools and analytical methodologies,\nspecifically tailored for LLMs, remains largely uncharted territory.\nFurthermore, the focus of current works gravitates towards func-\ntionality rather than system performance , leaving large room for\nenhancement in efficiency, space management and scalability. Note-\nworthy shortcomings include reliance on single-machine Python\nscripts, inappropriate handling of large-scale data, and poor pro-\ncessing speeds due to the utilization of Python’s plain dict object.\nWe will provide further empirical comparisons in terms of both\nthe quality of the generated data recipes (Sec. 7.1) and the perfor-\nmance of the data processing system (Sec. 7.2).\n3 STANDARDIZED OPERATOR POOL\nIn addressing the heterogeneity of data recipes for LLMs (Chal-\nlenge 1 in Sec. 1), we devise a set of standardized operator (OP)\npool. As outlined in Table 1, the OPs are organized into four primary\ncategories: Formatters, Mappers, Filters, and Deduplicators, which\nincorporate diverse categories, functions, inputs, processing levels,\noutputs, and application scenarios. Core principles of decoupling\nand composability guide their structuring, resulting in a varied\nyet standard set of procedures that contribute to flexibility and\nuser interaction at multiple processing levels. This strategic im-\nplementation enhances reusability and reduces complexity, aiding\nstreamlined and decoupled data recipe construction.\n3.1 Unified Data Representation\nWe first introduce Formatter OPs designed to unify diverse data\nsources into an intermediate data representation. Specifically, we\nchoose to build Data-Juicer upon Huggingface-datasets [55] due\nto its compatibility with mainstream LLM datasets and its column-\noriented storage ability backed by Apache Arrow[2]. Our Format-\nters maintain data objects that are instantiated from several unified\nbase classes that simplify the process design for follow-up OPs and\nfacilitate data accessing efficiency. We support numerous text input",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 3
      }
    },
    {
      "content": "Table 1: Overview of the operator (OP) pool in Data-Juicer, with a detailed list continuously maintained at the official\ndocumentation: https://github.com/alibaba/data-juicer/blob/main/docs/Operators.md.\nCategory Function Input Process Level Output OP Usage Examples\nFormatters Data format\nunifying\nDataset Dataset Dataset Load and unify dataset-hub, txt, json, md, codes,\nhtml, pdf, docx, ...\nMappers In-place text\nediting\nSample Single-sample;\nMulti-samples\nSample;\nSamples\nTransform specified headers, textual elements; Fix\nmessy codes; Enable text enhancement\nFilters Conditional\ntext removing\nSample Single-sample;\nDataset\nBoolean Filter by meta-info, stats ( e.g., lines count); model\nscores; external resources (e.g., flagged words)\nDedup-\nlicators\nDuplication\nremoving\nSingle or\nPaired Dataset\nDataset Dataset Compare with hash-based and vector-based\ndeduplication methods\nformats - txt, JSON, parquet, html, md, pdf, code files such as .py\nand .cpp, amongst others - and homogenize them into a structured\nformat composed of certain columns with nested access support ,\nwhich are conceptually organized by three primary parts “text”,\n“meta”, and “stats”. These parts respectively hold the raw textual\ndata, metadata information (e.g., date and version), and statistical\ndata that can be generated and consumed by Data-Juicer’s other\nOPs and tools. This interface works at either the text sample or\ndataset level, and is independent of underlying in-memory or disk\ndata layout, alleviating the potential worry over heterogeneous\ndata formats by OP developers.\n3.2 Versatile Data Processing\nNext, we elaborate on the functionality of the OP pool inData-Juicer,\nwhich is pivotal to the comprehensive data processing tailored for\nLLMs. Besides the Formatters, which play an essential role in uni-\nfying data formats and ensuring a consistent and efficient data flow\nthroughout the processing pipeline, we now give more details about\nthe other three types of data-transformation OPs in Table 1.\nMappers facilitate crucial functionalities of in-place text edit-\ning, necessary for single-sample or multi-sample processing across\nvarious needs of LLM data processing, such as modifying texts\nfor pre-training and enhancing text diversity for fine-tuning. They\neffectively handle processing tasks like the removal of specific file\nheaders, messy code rectification, and text enhancements.\nFilters come into play by conditionally filtering texts via individual-\nsample metrics, dataset-level statistics, or external resources like\nstop-word lists. In doing so, they can eliminate unnecessary text\nsamples, contributing to data focus, cleanliness, and the cost reduc-\ntion of follow-up LLM training processes significantly.\nDeduplicators reduce potential storage waste and improve effi-\nciency. As indicated by several studies [13, 47, 52], duplicate samples\nadversely affect both the pre-training stability and the performance\nof LLMs. Besides, Deduplicators help prevent unintentional data\nleakage during training into evaluation benchmarks, particularly\nfor zero-shot or few-shot tasks [39]. To ensure accurate detection\nand removal of duplication, we provide efficient and robust methods\nincluding hash-based and vector-based comparisons [8, 14, 81].\nIt is noteworthy that the outputs of Filter OPs are Booleans,\nwhich helps to decouple the implementations of actual data process-\ning and computation for various statistics. This dedicated segrega-\ntion results in two key advantages. Firstly, it enables our dedicated\nanalyzer-related tools (detailed in Sec. 5.2) to utilize these computed\nstatistics for the entire dataset, rather than a filtered subset. Users\nare also allowed to generate fingerprints for specific partial sam-\nples. Secondly, this decoupling enhances compatibility between\nHuggingface-datasets and Data-Juicer, thereby enabling the effi-\ncient reuse of the Dataset.mapand Dataset.filterinterfaces to\nperform these sub-processes in a streamlined manner. As a result,\nusers can effortlessly extend their own custom OPs that only vary\nfrom existing OPs in specific partial processing behaviors. In Ap-\npendix A.1, we offer an illustrative code example of this decoupling\nin Listing 1.\n3.3 Composability\nData-Juicer’s OPs serve as a testament to our system’s versatility.\nThey enable users to effortlessly process a variety of data types\nin a composable and modular manner, showcasing Data-Juicer’s\ndedication to user adaptability and high-quality data production\nfor LLMs. Besides the functions, inputs, outputs and processing levels\nsummarized in Table 1, this composability is embedded in more\nfacets, including the fields to be processed, OP hyper-parameters, and\nrecommended use cases of each OP.\nEach OP in Data-Juicer is designed to serve a distinct function\nand can be commanded by users to process different text fields. For\nexample, OP A could process the sample field “text.abstract”, while\nOP B could focus on “text.main_body”. By default, each OP process\non “text” field, which can be freely specified to other “meta” or\n“stats” related data fields according to users’ needs. This adaptability\nallows for immense flexibility by simultaneously using OPs with\ndifferent fields, enabling users to easily manipulate specific text\nsnippets such as removing GitHub codes based on their star counts.\nMoreover, these OPs establish a one-size-fits-all solution that\nencompasses a multitude of configurable parameters such as the\nnumber of tokens, filtering thresholds, auxiliary models, and much\nmore. This adjustability of a single OP, amalgamated with the com-\nposability of OP pipelines, empowers Data-Juicer to manage a\nspectrum of input, output, and processing granularity, contributing\nto its powerful processing abilities.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 4
      }
    },
    {
      "content": "For usage combinations, OPs are labeled with typical usage sce-\nnarios. We maintain OP tags as general usage, LaTeX source files,\nprogramming codes, financial data processing, or language-specific\nprocessing such as English and Chinese, and so on. These labels\nfacilitate easy navigation and operation, underscoring our aim to\nblend simplicity with power in Data-Juicer’s architecture.\n4 FEEDBACK-DRIVEN DATA PROCESSING\nAddressing Challenge 2 outlined in Sec. 1, we incorporate a dynamic\nfeedback loop into the data processing pipeline, which allows users\nto process and understand data effectively via built-in visualization\nand automated tracking abilities. As demonstrated in Figure 2, our\nsystem (Data-Juicer) enables timely perception and swift iterative\nrefinement of data recipes (indicated by the left and upward arrows)\nwithin a holistic feedback loop of LLM data processing and LLM\ntraining (indicated by the right arrows).\nAuto-EvaluationHPO for recipe (+ Checkpoints & Cache)\nOne-stop Development: Data-in-the-LLMdev-Loop\nInteractive VisualizationInteractive Visualization\nData Recipe [built-in, custom]\nData Probe[analyser, visulizer]\n Data Processing\nData QualityAssement\nLLMs Training/ Tuning\nFigure 2: The feedback loop of Data-Juicer.\nWe will discuss the modeling of the data processing feedback in\na hyper-parameter optimization (HPO) perspective (Sec. 4.1), and\ngo through the utility of the interactive visualization (Sec. 4.2), and\nthe integration of ecosystems for LLM training and evaluations\n(Sec. 4.3). The synergy of these techniques offers an efficient and\neffective solution to debug and dive into LLM data processing.\n4.1 HPO for Data Processing\nIn Data-Juicer, we incorporate the concept of hyper-parameter\noptimization (HPO) into the data processing procedure. This is done\nby tying data-processing-specific hyper-parameters to a variety of\nfeedback signals, including custom target metrics and visualization\nresults. We enhance our system’s functionality by innovatively\nspeeding up the data processing iteration through Checkpoint and\nCaching mechanisms, and by integrating an automated HPO tool.\n4.1.1 Acceleration with Checkpoint and Caching. LLM data\nprocessing often necessitates frequent re-conduction due to the al-\nterations in OP hyper-parameters and potential conduction failures,\nsuch as exceeding available memory, disk or pre-defined time limits,\nespecially for massive datasets. Accordingly, we provide built-in\ncheckpoint and caching management to foster resilient and reliable\ndata processing. Based on a carefully organized directory structure,\nData-Juicer automatically monitors every running process for\nconfiguration changes, and creates new files to safely store data and\nprocessing states only when any error or exception occurs. While\nthe checkpoint preserves the whole dataset and processing state\nenabling complete recovery of the processing site, the cache solely\nsaves the dataset object for each OP and is more suited for smaller-\nscale adjustments as it reduces the overhead of pre-order caches.\nThese techniques allow for a swift recovery during system restarts\nor failures, reverting to the most optimal recent processing state\nstored in the checkpoints, thus mitigating processing redundancy\nand increasing the feedback frequencies.\nAdditionally, the proposed state-saving mechanism enables a\nflexible space-time trade-off at different feedback stages. Users have\nthe option to save states after each OP in the data processing flow,\nensuring minimal re-execution time at the cost of maximum storage\noverhead. Conversely, they could choose to only save the last OP’s\ncheckpoint and cache, incurring minimal storage overhead but\nincreased re-execution time, especially when needing to revert to\nearly steps in the process.\nTo facilitate a good space-time trade-off, we further perform\nspace complexity analysis for individual OPs, which aids in pre-\ndicting peak space occupancy and guides us in determining how\nmany checkpoints and caches to store based on available space.\nBy default, Data-Juicer actively monitors disk space usage at the\nstart of data processing, and automatically determines if, and when,\ncheckpoints and cache should be deployed. User-specified saving\nfrequencies and rules are also supported. Consequently, strategic\ncheckpoint and cache management reinforce both the resilience\nand efficiency of the feedback loop for LLM data processing. The\ndetailed space usage analysis can be found in Appendix A.2.\n4.1.2 Auto-HPO. We incorporate an automated HPO tool1 into\nData-Juicer to streamline the finding of good data processing\nhyper-parameters. To reduce search costs of different data recipes,\nwe support leveraging advanced HPO algorithms such as Bayesian\noptimization [82], progressive early-stop strategies, such as the Hy-\nperband algorithm [56], and built-in LLM-oriented sampling strate-\ngies (detailed later in Sec. 5.2). Specifically, given a pre-defined tar-\nget metric and search space of data recipes, users can conveniently\nexplore the impact of specific data processing hyper-parameters.\nHere, we give an illustrative example as follows:\nExample of Data Mixing with HPO:\nSuppose we aim to find a good set of sampling weights for 𝑀\ndatasets to be mixed, where our search space is defined as 𝑤𝑖 ∈\n[0,1],𝑖 ∈[1,𝑀]. The pipeline can be structured as follows:\n(1) We specify the target text fields across all𝑀datasets, and unify\ntheir meta-tags and name of text fields via Formatter OPs.\n(2) We leverage meta-tag Filters to cater to specific usage scenarios.\nHere we only include samples with the language tag “EN”.\n(3) A datasets D𝑚𝑖𝑥 is generated from the𝑀datasets, with mixture\nweights [𝑤𝑖]drawn by the HPO scheduler to maximize the\ntarget metric in step (5).\n(4) A pre-configured data processing including de-duplication OPs\nis executed on the mixed dataset, ensuring dataset cleanness.\n(5) The target metric is calculated on D𝑚𝑖𝑥 as (𝑛/𝑁 +𝑠), where 𝑁\nis the total number of tokens of all 𝑀 datasets, 𝑛and 𝑠 is the\nnumber of tokens and average quality score ofD𝑚𝑖𝑥 using built-\nin GPT-3 quality classifier (detailed in Sec. 5.2) respectively.\nThe mixture dataset D𝑚𝑖𝑥 is iteratively refined by carrying out it-\nerations steps (3)∼(5) to get a larger quantity and better quality. □\nThe HPO results offer a powerful means of visualizing and under-\nstanding data insights as shown in Figure 3, where the importance,\n1W&B Sweeps, https://docs.wandb.ai/guides/sweeps",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 5
      }
    },
    {
      "content": "Linear CorrelationGlobal Interactions\nHigh-order Correlation\nFigure 3: Demonstration of HPO for data recipe.\n(a) Tracking Specific Data Samples\n(b) Effect of OP Pipeline (Number of Samples)\nbeforeafter\nalnum_ratio (c) Data Distribution Diff.\nFigure 4: The illustration of interactive visualization of\nData-Juicer. More demos are publicly available.\ncorrelation and interaction of 𝑤𝑖 for the quality score are estimated\nand plotted. Besides the quality score demonstrated in this exam-\nple, the target metric can be customized to include other trade-off\nterms composed of intrinsic data measures – such as toxicity, help-\nfulness, or other scores predicted by auxiliary models – or even\nperformance measures of LLMs, such as training loss or benchmark\nscores (which we will discuss later in Sec. 4.3).\n4.2 Interactive Visualization\nThe ability of interactive visualization is integral to multiple feed-\nback stages of Data-Juicer. Specifically, as Figure 4.(a) demon-\nstrates, users can visually track the effects of individual OPs in terms\nof the processed data samples. This is facilitated by an innovative\nbuilt-in tool, tracer, which records sample changes after apply-\ning each operation for Data-Juicer. For example, tracer presents\ndiscarded samples for Filters, pre- and post-editing differences for\nMappers, and (near-) duplicate sample pairs for Deduplicators. Cou-\npling this tracking ability with fruitful built-in sampling and visu-\nalization tools, Data-Juicer enhances users’ control over the data\nprocessing and boosts their confidence and rationals of the process.\nTransitioning to the mid-term stage of LLM data processing,\nData-Juicer offers a comparative visualization of the data before\nand after the entire processing from the view of OP pipeline and sta-\ntistical analysis, as Figures 4.(b) and 4.(c) show. Aided by a built-in\ntool, analyzer, Data-Juicer provides statistical analysis (counts,\nmeans, standard deviations, min/max, quantiles, entropy, etc.) to\nallow a deep understanding of the data. By default, the summary\nof per-sample statistics covers 13 dimensions and automatically\ndisplays histograms and box plots for each statistical variable, in-\ncluding diverse criteria like sample perplexity, word count, flagged\nword percentage, and paragraph length, among others. Users also\nhave the flexibility to adjust the dimensions to observe, with a\nbespoke visualization and data processing experience.\n4.3 Feedback with Integrated LLM Libraries\nIn the later stages of our pipeline, we utilize robust ecosystems\ndesigned for LLM training and evaluation, ensuring seamless in-\ntegration with widely-used libraries such as Megatron-LM [ 85],\nDeepSpeed [78], and HuggingFace’s Transformers [101]. With this\nintegration, users can easily train LLMs on datasets produced by\nData-Juicer and evaluate their performance to obtain feedback\nusing our pre-built tools and scripts, without getting bogged down\nin complicated LLM training and evaluation details.\nNotably, our system facilitates the timely assessment of model\nabilities by incorporating multiple dimensions. The system’s capa-\nbility to swiftly identify potentially ineffective data and training\nallows us to terminate unwanted LLM data processing promptly.\nInstead of solely relying on model loss as the basis for evaluating\nmodel performance, we support the LLM assessment across various\nmetrics or benchmarks, and track shifts in target scores. Conse-\nquently, we can determine whether continued training of an LLM\non the produced dataset is justified, thereby helping us minimize\ndata processing and LLM training costs.\nSpecifically,Data-Juicer’s evaluator supports SOTA LLM bench-\nmarks such as HELM [ 59], LM-harness [32] and GPT-API-based\nevaluation [19], as well as the extension of customized evaluation\nbenchmarks and tasks. For a balanced and straightforward evalua-\ntion, Data-Juicer supports a leaderboard-style comparison by con-\nsolidating results from different target evaluation scenarios, such\nas ranking averaging, score-normalized averaging, or other cus-\ntomized strategies. The leaderboard-style scoring utility enhances\nthe visualization of strengths and weaknesses of models, guiding\nsubsequent iterations of data recipes and LLMs’ refinements. We\nalso make available Reference Models - these are model checkpoints\nbinding with traceable training data in Data-Juicer, popular LLM\narchitectures, training parameters, computation costs, and corre-\nsponding evaluation results. They facilitate effortless comparison\namong different training configurations, particularly for further\nresearch on diverse, iteratively developed data recipes.\n4.4 Feedback Loop Showcase\nThe general feedback loop has been discussed before in Figure 2. We\nnow further expound on this by presenting a concrete development\nexample. Here, we intertwine several previously mentioned tools to\ndemonstrate the Data-in-the-LLMdev-Loop process, which results\nin improved LLM data. As illustrated in Figure 5, we begin with a\nraw dataset and aim to refine it for better pre-training or fine-tuning\nof an LLM. The entire process flows as per the following steps:\n(1) Analyze the original dataset. We can opt to utilize an\nexisting data recipe (a specific configuration file) or craft a new\none based on prior understandings of data processing needs. Our\nbuilt-in Analyzer and Visualizer facilitate this process by computing",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 6
      }
    },
    {
      "content": "Refined Data Probe\nOriginal Data Probe\nprocess:  -word_repetition_filter:rep_len: 10min_ratio: 0.0max_ratio: 0.5  -special_characters_filter:      min_ratio: 0.0      max_ratio: 0.25  ……\nOriginal Recipe (Config File):\nOriginal Dataset process:  -word_repetition_filter:rep_len: 3min_ratio: 0.0max_ratio: 0.23  -special_characters_filter:      min_ratio: 0.07      max_ratio: 0.25  ……\nRefined Recipe:\nRefined Dataset\nAnalyze originalDataset(via Analyzer     & Visualizer)\n1\nRefine parameters of data recipe(manally or via HPO)2\nProcess data with refined recipe(reusing checkpoints & caches)3\nAnalyzerefineddataset\n4\nImproved Quality and Quantity\nData Leardboard with Reference Models\nImproved Diversity and Distribution\nReal-Time & Auto Evaluation\nTrain/Tune LLMs5\nCollate &   compare\n6\nMMLU.EM\nFigure 5: The demonstration of data processing feedback of Data-Juicer, which helps to generate better data recipes for LLMs.\nmore than a dozen measures such as linguistic diversity, textual\nstatistics, and others to generate a data probe. The two pie plots\nwithin Figure 5 indicate the top 20 most common root verbs (inner\ncircle) and their top 4 direct noun objects (outer circle) for the data\nin field “text.instructions”.\n(2) Refine parameters of the original recipe. Based on the\ndata probe, users figure out the weaknesses of the original dataset,\nsuch as low diversity in expression manners, and long-tail statistics\nof word counts. Then we refine the parameters in the recipe by\nadding/removing some OPs or tightening/relaxing filter ranges.\nDuring refining, we could find out the effect of each OP easily\nbased on the interactive visualization tool mentioned in Sec. 4.2.\n(3) Process the original dataset with the refined recipe.\nThen we process the original dataset with the refined recipe using\nData-Juicer and get a refined dataset and several saved check-\npoints for further adjustments. This step can be facilitated with the\nhelp of our cache and checkpoint mechanisms.\n(4) Analyze the refined dataset. Like step (1), we analyze the\nrefined dataset again to obtain a new data probe. Based on the statis-\ntics and visualization results, we assess the degree of improvement\nin the data quality. If the refined data fails to meet our expectations,\nwe revert to step 2 to manually adjust the data recipe or employ\nour HPO tool for automatic refinement (refer Sec. 4.1).\n(5) Get LLMs with the refined dataset. Then we can train or\nfine-tune LLMs with the refined dataset and training frameworks\nintegrated into Data-Juicer (Sec. 4.3). During the training or fine-\ntuning process, our auto-evaluation tools offer timely, multi-view\nassessments of LLMs. These tools inspect numerous metrics across\nmultiple evaluation datasets. This feature provides us the advantage\nof halting the process prematurely if the refined data weakens LLM\nperformance, thereby preventing unnecessary costs.\n(6) Collate results and compare with reference models.\nFinally, Data-Juicer automatically collates the evaluation results\nand compares them with reference models in the data leaderboard,\nproviding a clear representation of the effects of data processing\nalone. Consequently, we derive either a superior LLM, which can be\nauto-registered as a reference model, or additional refining guidance\nfrom the LLM perspective to further enhance data recipes.\n5 BOOSTING USABILITY WITH BUILT-INS\nIn response to the challenge of varied user customized preferences\nand technical expertise (Challenge 3 in Sec. 1), we offer an easy-\nto-use configuration paradigm for data recipes, ready-to-use data\nrecipe templates, and extensive tools, as detailed below.\n5.1 Configuring Your Data Recipe\nNotably, we make the end-to-end pipeline of data processing con-\nfigurable in Data-Juicer, including specified processing environ-\nment parameters, OP lists, tools used, and so on. This principle of\nall-in-one configuration ensures reproducibility and traceability,\nand simplifies changing specifications in data processing, thereby\nfacilitating the formation of data recipes for further refinement\nand reuse, and enabling the quantitative exploration and automatic\noptimization of data processing (Sec. 4.1).\nSpecifically, built upon Jsonargparse [46], we provide unified,\nflexible, easy-to-use and powerful configuration capabilities. It is\nengineered to automatically register configuration items for OPs\nand tools, and accept varying sources of configurations such as com-\nmand line entries, yaml and jsonnet files, environment variables,\ndefault hard-coded values, and a mixture of those for convenient\nincremental modifications.\nFor example, users can easily build up their own config files by\ntwo recommended methodologies—“subtraction” or “addition”. The\n“subtraction” approach utilizes a pre-set configuration file contain-\ning all available OPs, tools, and their default parameters . Users\ncan simply remove or re-order these OPs and adjust these parame-\nters per their requirements. Conversely, the “addition” approach\nlets users build their configuration files from scratch, leveraging our\nextensive examples of pre-built data processing recipes for totally",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 7
      }
    },
    {
      "content": "more than 20 high-quality and diverse data recipes for pre-\ntraining, fine-tuning, English, Chinese, etc. More quantitative\nanalysis on certain recipes are in our experiments (Sec. 7.1).\n5.2 Dedicated Pluggable Tools\nTo further enhance usability, facilitate system customization and\naugment users’ data handling capabilities, Data-Juicer includes\nan extensible collection of powerful dedicated tools that can be con-\nveniently plugged into different stages of the LLM data processing.\nQuality Classifier. As an illustrative example, we describe our\ntext quality classifier for culling high-quality text from heteroge-\nneous data sources like CommonCrawl. This tool is a reproduced\nmodel based on the closed-source GPT-3 quality scorer [9]. More-\nover, we have expanded its applicability to Chinese text and various\ncode types. Encapsulated as a callable pipeline, this tool provides\nusers with the freedom to adapt it to other various scenarios.\nThe functionality of the classifier is backed by PySpark’s standard\nTokenizer or Sentencepiece model [50], along with HashingTF as\nthe feature extractor. It then applies a binary logistic regression\nclassifier to gauge the quality of a text. We provide more empirical\nverification of them in Sec. 7.2.3.\nEnhanced Sampler for LLM data. In Data-Juicer, we have\ndesigned several advanced data sampling utilities specialized for\nlarge-scale data chunk handling in LLMs. Our solutions effectively\nstreamline representative extraction, optimize processing time and\nresources, and meet the distinctive needs of LLM developers.\nOur stratified sampling technique is noteworthy in this LLM\ndata context. It capitalizes on information within the metadata\nor statistical fields, thus accommodating varied selection metrics\nin crafting an effective data sample. To ensure a comprehensive\nyet flexible representation of the data corpus, we consider various\nheterogeneous criteria such as document length, token count, the\nfrequency of boolean predicates for post-conditional checks, and\neven linguistic diversity formulated via occurrences of verb-noun\npair (as shown in the pie plots in Figure 2) . These dynamic criteria\nare tailored to distinct analytic needs and promote efficient data\nprocessing, seamlessly integrating with downstream OPs and tools.\nFull Toolkit. As for other tools, readers can refer to Sec. 4 for\nan examination of multiple previously discussed tools, including\ntracer and analyzer (Sec. 4.2), and evaluator and reference mod-\nels (Sec. 4.3). We diligently maintain and evolve the toolkit in\nData-Juicer, and make the full set publicly accessible.\n5.3 User-Friendly Experiences in Data-Juicer\nData-Juicer is designed not just for functionality but also for\nadaptability, catering to an extensive user base with diverse exper-\ntise and skill sets. While abstracting the intricate system internals,\nwe provide user-friendly interfaces and extensive customizable\ncomponents. Accordingly, users can embark on zero-code data pro-\ncessing, engage in low-code customization, or delve into in-depth\nextensions for complex requirements.\n• Zero-Code Processing: For novice users, Data-Juicer sup-\nplies a series of ready-to-use data recipes and plug-in tools for\nimmediate use. This requires no knowledge of advanced system\narchitectures or OPs, as discussed in Sec. 5.1 and Sec. 5.2.\n• Low-Code Customization: Intermediate users can enjoy the\nflexibility to alter configurations, data, and external resources to\nsuit their specific needs. They can readily reuse, combine, and\nedit built-in data configurations; customize quality classifiers\nand tokenizers; refine data based on our pre-developed recipes;\nor provide fresh links to auxiliary models or vocabularies from\nour unified, routinely updated public cloud drive.\n• Advanced Extension: Experienced users are allowed to easily\nintroduce new OPs by deriving from base classes and implement-\ning their specific “process()” and “compute_stats()” functions,\nas demonstrated in the code Listing 1. This grants the users\nan end-to-end view of the process for a single sample, while\nData-Juicer handles the nitty-gritty of configuration registra-\ntion and efficiency optimization.\nAdditionally, Data-Juicer’s decoupled design facilitates the\nsmooth incorporation of new tools for users at all stages of LLM\ndata processing, ranging from novel visualization dimensions and\nevaluation datasets to pre- or post-processing scripts.\nTo enhance the ease of adoption and use of Data-Juicer, apart\nfrom the numerous pre-built data recipes (refer Sec. 5), we also\nprovide a series of interactive demos, implemented in Streamlit,\nfor varied profiles and scenarios. This hands-on learning approach\nhas been designed to enable users of varying skill levels to quickly\nfamiliarize themselves with and effectively use Data-Juicer.\n6 COMPREHENSIVE SYSTEM OPTIMIZATION\nTo handle large-scale data (Challenge 4 in Sec. 1), we employ a\nseries of optimizations in Data-Juicer from various aspects.\nOptimized Computation: Context management, Operator\n(OP) Fusion and Reordering. To elevate computational efficiency\nin LLM data processing, we provide advanced context management,\noperator fusion, and operator reordering techniques for nuanced\nimplementation contributions. The manager meticulously handles\nshared intermediate variables, such as segmented words, split lines,\nand others derived from the original textual corpus, across different\noperators. It allows seamless reuse of these context variables across\nmultiple operators, thereby mitigating the necessity for computa-\ntionally expensive re-evaluations.\nBased on the context manager, the proposed operator fusion\nmethod is another new contribution to the field. We propose to\nidentify fusible operators that either share the same contexts or\ncomputation sub-procedures. It detects the OP groups first. Succes-\nsive OPs in the same group should be commutative with each other.\nIt then amalgamates identified fusible operators in each group into\na single fused OP, enabling them to be executed faster with a larger\nlocalized perspective. The contexts of each sample will be cleaned\nup after each fused OP, hence little extra memory is required for\ncontext management and operator fusion.\nDue to the time-consuming increase of single fused OP, we fur-\nther design a strategy of operator reordering to optimize the execu-\ntion sequence of the OP list after fusion. For example, based on the\ncommutativity of Filters, we delay the running of time-consuming\nOPs (such as fused Filters) and prioritize other less time-consuming\nOPs. As a result, these time-consuming OPs only need to handle\nfewer samples because the preceding operators have filtered out\nsome of them, enhancing overall computational efficiency.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 8
      }
    },
    {
      "content": "FilterFusible FilterMapperDeduplicator\nOP Types\n123456789101112\n123456789101112\nGroup1\nGroup2\nGroup3\nFind filtergroups\nReorder the only1 fusible OP\nFuse >1 fusibleOPs and reorder\nDo nothing to0 fusible OPs\n21347\n9101112\nFused OP(5, 6, 8)Common Contexts\nGroup1\nGroup2\nGroup3\nFigure 6: The OP fusion procedure for an OP list.\nThe whole procedure of OP fusion is summarized in Figure 6.\nThese amalgamation strategies serve dual purposes. Firstly, it mini-\nmizes redundant computation, eliminating the need for repetitive\nyet shared computations. Secondly, it mitigates the overhead of\ninitializing multiple processes by reducing the total count of pro-\ncessing OPs, thus maintaining expeditious data processing routines.\nOptimized Space Utilization: Caching OPs and Compression.\nRecognizing the inadequacies of the original cache management\nprotocol in the Huggingface-datasets library, especially pertaining\nto the handling of non-serializable third-party models and functions\nin certain OPs, we design a dedicated hashing method to bypass the\nserialization procedures of those non-serializable objects, which\nensures successful caching of each OP and permits Data-Juicer\nto leverage optimal cache management.\nFurthermore, we incorporated the ability for users to activate ad-\nvanced compression technologies, such as Zstandard (zstd) [23] and\nLZ4 [64], in Data-Juicer. It will automatically compress cache files\nafter each OP and decompress these compressed files back to nor-\nmal cache files when rerunning this OP in the same configuration.\nCompared with the processing time, compressing/decompressing\ntime is relatively negligible due to the high efficiency of the com-\npression technologies mentioned above. This feature substantially\nreduces the volume of cache data storage, facilitating the processing\nof larger datasets without compromising speed or stability.\nOptimized Scalability: Distributed Data Processing. The vol-\nume of LLM training data can be extremely large, making it difficult\nto be processed with a single machine. Data-Juicer meshes with\ndistributed processing frameworks such as Ray [66], Apache Beam\n[5] and Apache Flink [12], and offers the ability to seamlessly trans-\nlate a data processing pipeline running on a single node into a\nmulti-node cluster. In this way, resources in cluster computing can\nbe utilized to accelerate data processing and generation.\nSpecifically, we adapt the underlying interfaces of HuggingFace-\ndatasets for those ofRay-datasets, such that all OPs ofData-Juicer,\neven when written as single-machine Python functions, can be\nexecuted in a distributed mode with the help of automatic data\npartitioning by Ray. An alternative approach we support is to\nreplace the default Ray runner of Data-Juicer with other dis-\ntributed processing back-ends such as Flink, via pre-translations\nfrom Data-Juicer’s processing pipelines into theBeam-compatible\nones. As a result, almost all the OPs within Data-Juicer (Mapper,\nFilter, and Deduplicator) can be accelerated in a multi-node clus-\nter, and effectively alleviate the bottlenecks on a single node (even\nwith process-based parallelism) caused by memory capacity and IO\nthroughput. More empirical results can be found in Sec. 7.2.4.\nIn a nutshell, all of these optimizations enhance Data-Juicer’s\nscalability from various views, to handle the vast amount of data\ninvolved in LLMs, ensuring robust and efficient processing while\nminimizing resource requirements.\n7 EVALUATION OF DATA-JUICER\n7.1 Making Better Data Recipes\nThe value of an effective LLM data processing system is reflected\nnot only in its comprehensive and flexible operability but also\nin its capacity to produce high-quality data that LLMs can more\nreadily “digest”. Data-Juicer provides specialized features for ex-\nploring and making data recipes tailored to LLMs, and we have\ndeveloped numerous ready-to-use data recipes using Data-Juicer.\nIn this section, we evaluate the quality of data recipes generated by\nData-Juicer for both LLM pre-training and fine-tuning.\n7.1.1 Refined Pre-training Data Recipes. The pre-training data\nwe produced consists solely of publicly available sources, exem-\nplifying the core principles of transparency and reproducibility.\nSpecifically, we choose to improve two widely-used, high-quality\ndatasets for LLMs, TogetherAI’s RedPajama [24] and EleutherAI’s\nPile [31], which were curated from 15 highly diverse text sources\nand subjected to meticulous pre-processing and cleaning to ensure\ntheir quality. With the help ofData-Juicer, we further refine them\nvia data analysis, merging and quality enhancement, employing\ndozens of OPs with varied configurations. For detailed statistics,\nprocessing steps and refined data recipes, please refer to Appendix\nB.2.\nTo verify the quality of the data recipes derived byData-Juicer,\nwe use the original RedPajam and Pile, and our refined datasets to\npre-train LLMs with mainstream LLaMA architecture and assess\nthe models’ performance across 16 core HELM tasks. We keep the\ntraining configurations the same while only modifying the training\ndata. Detailed hyper-parameters are in Appendix B.3.1. The results\nof average scores of 16 tasks are visualized in Figure 7, where we\nevaluated checkpoints throughout the pre-training process with\nan increasing number of billion-sized tokens at 50B, 100B, and\n150B. Notably, through fair comparisons with equivalent training\ntokens, LLMs pre-trained onData-Juicer-recipes consistently out-\nperformed those using only RedPajama or its union with the Pile,\nreinforcing the usefulness and effectiveness of Data-Juicer.\nMoreover, we compareData-Juicer-models with several SOTA\nbaselines and summarize the results in Table 2. With only half the\ndata volume (150B tokens), LLaMA-1.3B pre-trained onData-Juicer-\nrecipe outperformed Pythia-1.4B [6] (300B tokens), and even beats\nhighly competitive Falcon-1.3B [71] trained on 350B tokens. No-\ntably, we further labeled 17 subsets from Alpaca-CoT (a collection of\n39 public fine-tuning datasets) with the “Instruct Fine-Tuning (IFT)”\ntag and performed data mixing and processing using Data-Juicer.\nFollowing the usual practice [ 105], we incorporate these large-\nvolume IFT data into the pre-training phase and execute continuous",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 9
      }
    },
    {
      "content": "50 75 100 125 150\n#T okens (B) for pre-training LLaMA-1.3B\n29\n30\n31\n32\n33\n34\n35\n36Average score on 16 tasks\n29.57\n32.29\n30.30\n30.63\n32.14\n32.89\n31.21\n33.07\n34.21\nRedPajama+Pile (Data-Juicer)\nRedPajama+Pile\nRedPajama\nFigure 7: Evaluation results of reference models trained\nwith different datasets but the same pre-training procedures.\nData-Juicer’s data recipe gains consistent improvements\nover baselines.\ntraining upon the checkpoint of Data-Juicer (RedPajama+Pile)-\n150B. As reflected in the last two rows of Table 2, Data-Juicer\ngains a further 4.9% relative improvement over the original Alpaca-\nCoT-IFT while utilizing only ∼30% data volume.\nTable 2: The average score of the pre-trained LLMs on the 16\nHELM core tasks. Individual task results and data recipes are\ndetailed in Appendix B.4. “IFT” denotes the datasets tagged\nwith “Instruct Fine-Tuning” in our context.\nModel Training Data #Tokens Score\nFalcon-1.3B [41] RefinedWeb 350B 33.97\nPythia-1.4B [29] Pile 300B 33.96\nLLaMA-1.3B\nData-Juicer\n(RedPajama+Pile) 150B 34.21\n+ Alpaca-CoT-IFT 150B + 15B 35.04\n+ Our Refined IFT 150B + 4.7B 36.76\nTaken together, these findings underscore the potential of the\nData-Juicer system to generate high-quality data and verify the\nexcellence of Data-Juicer-recipes in terms of enhancing LLM\nperformance while reducing LLM training costs.\n7.1.2 Refined Fine-tuning Data Recipes. For the Alpaca-CoT\ncollection, besides the “IFT” tag as validated in Table 2, we also\nlabeled datasets within it with “Chat Fine-Tuning (CFT)” for en-\nhanced dialog ability and aligned human value. To examine their\nquality, we first use the CFT and EN tags to filter out several com-\npetitive subsets, and then generate two new equal-size datasets by\nrandom sampling and our designed recipe respectively. Then we\nconduct fine-tuning on the generated datasets based on the open-\nsource mainstream architecture, English LLaMA-7B [34]. Similarly,\nwe replace the tag “EN” with “ZH”, and use a SOTA LLaMA-2-7B\nvariant [42] for the Chinese scenario. Statistics of these datasets\nand training hyper-parameters are in Appendix B.3.2.\nFor a thorough and comparative performance evaluation, we\nused GPT-4 API for pairwise scoring and tallying of wins and ties.\nTable 3: Results of pair-wise model comparisons using GPT4\nscoring. “CFT”, “EN” and “ZH” indicate meta-tags as Chat\nFine-Tuning, English, and Chinese text respectively.\nModel Tuning Data #Samples Win Tie\nLLaMA-7B\n[34]\nAlpaca 52k 16\n100\nData-Juicer 40k 44\nRandom (CFT, EN) 40k 19\n105\nData-Juicer 40k 36\nLLaMA2-7B\n(Chinese,\nFlagAlpha\n[42])\nBelle 543k 28\n99\nData-Juicer 52k 33\nRandom (CFT, ZH) 52k 19\n96\nData-Juicer 52k 45\nThe results are consolidated in Table 3, from which we can see\nthat LLMs utilizing Data-Juicer-recipes consistently demonstrate\nhigh validity. Firstly, compared to LLMs trained on the competitive\nfine-tuning open datasets, Alpaca [92] and Belle [45], LLMs trained\non Data-Juicer data gain higher win rates (up to17.5% for English\ncase) while using less data (up to 90.4% reduction for Chinese case).\nSecondly, compared to the LLMs trained on the datasets with trivial\nprocessing strategy (mixture by random sampling), LLMs trained\non Data-Juicer still gain higher win rates (up to 14.4% ), which\nattests to the effectiveness of our enhanced sampling strategy and\nquality of Data-Juicer-recipes for LLMs again.\n7.2 Processing Data Efficiently and Effectively\n7.2.1 End-to-End System Performance. To evaluate the pro-\ncessing performance of Data-Juicer, we compare it with two\nSOTA baselines: TogetherAI’s RedPajama [24] and AllenAI’s Dolma\n[86]. A more detailed introduction to and comparison with these\nbaselines can be found in Appendix B.3.4. For a fair comparison,\nhere we use their official code repositories and run Data-Juicer\non the data recipes with the same OPs to process the Books, arXiv,\nand C4 datasets, which vary in terms of data sizes, distributions\nand involve diverse processing OPs.\nWe conduct multiple rounds of experiments on different numbers\nof processes (np=[32, 64, 128]) and monitor several core metrics, in-\ncluding processing time and average memory usage. The monitored\ntime is the wall-clock time of the whole processing pipeline. The\naverage memory usage is monitored every second and aggregated\nacross all relevant processes. For more experimental details, please\nrefer to Appendix B.3.3.\nThe experimental results are summarized in Figure 8. Notably,\nfor all datasets and various numbers of processes, Data-Juicer\nrequires an average of 50.6% less processing time and 55.1% less\nmemory. In particular, it saves at most 88.7% processing time for\nthe arXiv dataset compared with the baseline. Also, it takes up\nto only 22.9% memory of baseline for Data-Juicer to process the\nBooks dataset, which is mainly because the processing procedure of\nthe baseline loads the whole dataset at once. Overall, Data-Juicer",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 10
      }
    },
    {
      "content": "Figure 8: Comparison of stand-alone performance in various\ndata sizes and processing configurations.\neffectively alleviates the bottleneck caused by IO of cache files, and\nachieves better end-to-end time-space efficiency than baselines.\n7.2.2 Effect of Context Management, OP Fusion, and Re-\nordering. As introduced in Sec. 6, Data-Juicer employs dedi-\ncated optimization to minimize redundant computations and save\nprocessing time. To examine the optimization effect, we prepared\nthree test datasets of varied sizes and sample counts. Each dataset\ngoes through the same processing recipe which includes 14 OPs (5\nMappers, 8 Filters, and 1 Deduplicator), with 5 of these OPs being\nfuse-able. We conduct comparison experiments with 4 processes,\nexcept for the largest dataset, where we utilize 50 processes to\nassess if these techniques remain effective on larger scales.\n17MB- p=4 169MB- p=4 21GB- p=4 21GB- p=50\nDifferent dataset sizes and number of processes\n0\n20\n40\n60\n80\n100Normalized time consumption (%)\nn n n n\n100.00%\n25.33s\n100.00%\n176.47s\n100.00%\n27717.48s\n100.00%\n4507.79s\n75.09%\n19.02s\n86.59%\n152.81s\n79.22%\n21958.28s\n83.74%\n3774.75s\n55.50%\n49.27% 47.50%\n41.29%\n32.17%\n35.63%\n28.52% 25.63%\n24.91% 13.41% 20.78% 16.26%\n42.04%\n27.67% 39.97%\n37.91%\nAll OPs before fusion\nAll OPs after fusion\nFusible OPs before fusion\nFusible OPs after fusion\nFigure 9: Time comparison before and after OP fusion.\nThe results are shown in Figure 9, where both the normalized and\nactual time consumption for each experimental setup are indicated.\nThe results signify that our optimization strategy effectively saves\nup to 24.91% of the total time for the entire process and saves at\nmost 42.04% of time for those fusible OPs. In addition, the findings\nshowcase that the optimization performs efficiently regardless of\nvariations in dataset sizes or the number of processes utilized.\n7.2.3 Effect of Quality Classifiers. As described in Section 5.2,\nData-Juicer provides built-in quality classifiers for LLM data pro-\ncessing, and here we present several empirical results regarding\ntheir performance. Specifically, we follow the training procedure of\nthe proprietary quality classifier used in GPT-3 [9] and extend its\ntraining pipeline to include Chinese text. In the evaluation of the\ncollected data, we found that our reimplementation of the GPT-3\nclassifier and its Chinese adaptation achieved F1 scores of 97.47%\nand 98.64%, respectively. Further training and evaluation details\nare provided in the Appendix B.1.\nTable 4: Comparison of keeping ratio on CommonCrawl.\nQuality\nClassifier\nKeeping Ratio\n@ label\nKeeping Ratio\n@ Pareto\nOriginal GPT-3 - 1.30%\nOur GPT-3 3.22% 1.41%\nChinese 1.81% -\nFurthermore, we assess the filtering effectiveness of these clas-\nsifiers by comparing their keeping ratios on CommonCrawl. The\nresults are summarized in Table 4, where we employ two data keep-\ning methods used in GPT-3: (1)label: 𝑑𝑜𝑐𝑠𝑐𝑜𝑟𝑒 > 0.5; and (2)Pareto\n[9]: 𝑑𝑜𝑐𝑠𝑐𝑜𝑟𝑒 > 1 −np.random.pareto(𝛼),𝛼 = 9. The keeping ratios\nof our re-implemented GPT-3 quality classifiers are generally in line\nwith the original one, and our Chinese extended version maintains\na keeping ratio comparable to that of the English version.\n7.2.4 System Scalability. To verify the enhanced scalability of\nour system (as detailed in Sec. 6), we carry out a series of exper-\niments to measure data processing times across multiple servers.\nSpecifically, we adopt the StackExchange and arXiv datasets from\nRedPajama. The total size of the StackExchange and arXiv datasets\nare 65GB and 140GB in jsonl format, respectively. We compare\nthe performance of Data-Juicer on Ray, Data-Juicer on Beam\n(using the Flink backend), and original Data-Juicer in these tests.\nMore details about the implementation and experimental platforms\nare in Appendix B.3.5.\n1 2 4 8 16\n1024 cores\nNumber of nodes\n128\n256\n512\n1024\n2048\n4096\n8192\n16384Time (s)\nStackExchange\nStackExchange [Ray]\nStackExchange [Beam]\narXiv\narXiv [Ray]\narXiv [Beam]\nFigure 10: Processing time with varying number of nodes.\nData-Juicer accelerates processing in distributed mode.\nThe experiment results are illustrated in Figure 10. Notably,\nthanks to various optimizations, our original system outperforms\nboth Ray and Beam in the single server scenario. Moreover, as the\nnumber of nodes increases, the processing time of our system on\nRay decreases proportionally (up to 87.4% and 84.6% time reduc-\ntion on StackExchange and arXiv respectively), demonstrating its\neffective scalability across multiple servers.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 11
      }
    },
    {
      "content": "Nonetheless, the processing time of Data-Juicer on Beam re-\nmains almost unchanged as the number of nodes increases. Upon\nfurther investigation of the processing workflow, we found that\nthe limited scalability of Data-Juicer on Beam is primarily con-\nstrained by the data loading component of Beam, which leads to a\ndominant file loading time ratio and requires substantial develop-\nment changes for adaptation and further performance optimization.\n7.3 Empowering Real-world Products\nData-Juicer has been adopted by several real-world LLM-based\nproducts, playing a crucial role in data understanding and pro-\ncessing. It evolves continually through the integration of feedback\nfrom real-world demands. A notable testament to its utility is its\ncontribution to the development of several industrial LLMs from\nAlibaba Cloud’s Tongyi suite [21], such as Dianjin, which is used for\nfinancial analysis; Zhiwen, a reading assistance tool; and Xingchen,\nwhich specializes in AI character customization. Moreover, the data\nprocessing capabilities of Data-Juicer have been incorporated\ninto Alibaba Cloud’s Platform for AI (PAI) [ 22] to support more\nreal-world applications.\nOur system’s fine-grained OP abstraction, coupled with the ex-\ntensive tools for LLM data-processing, empowers users to easily\nexplore and refine data recipes tailored to the distinct textual at-\ntributes of diverse use cases. For example, within the financial sector,\nit is crucial to accommodate data that includes numerous digits\nand standardized terminology. In the realm of reading assistance,\nthe focus shifts to data characterized by extended text lengths and\ncoherent structures. Conversely, character customization demands\ndata rich in dialogue and varied enough to support personalized\nservices. Data-Juicer adeptly meets these varied demands by fa-\ncilitating the combination of distinct OPs, hyper-parameters, and\ntools that adapt to the unique need of each real-world application.\n8 CONCLUSIONS\nTo conclude, the introduction of Data-Juicer reflects a new step\nforward in the field of data-centric LLM development. By offering\na user-friendly, versatile, and efficient solution, Data-Juicer effec-\ntively addresses the existing limitations of open-source tools for\nLLM data processing, which lean towards data reproducibility at the\nexpense of adaptability and usability. The decoupling of tradition-\nally linked components fosters greater abstraction and modularity,\nand the organic arrangement of over 50 built-in operators, dedi-\ncated tools, and abundant data recipes serves diverse needs for LLM\npre-training and fine-tuning. Beyond supporting auto-evaluation,\nData-Juicer is carefully optimized and seamlessly integrated with\nboth ecosystems for LLM training and evaluation, as well as dis-\ntributed computing. Empirical validation bears witness to substan-\ntial improvements in LLMs’ performance usingData-Juicer’s data\nrecipes, and shows advances in system efficiency and scalability. As\nsuch, Data-Juicer stands as a compelling addition to the toolkit\nfor LLM data processing, which we hope can shed light on broader\nresearch for the field of data-centric LLM development.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 12
      }
    },
    {
      "content": "REFERENCES\n[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cap-\npelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow,\nJulien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and\nGuilherme Penedo. 2023. Falcon-40B: an open large language model with\nstate-of-the-art performance. (2023).\n[2] Apache Arrow. 2023. https://arrow.apache.org/\n[3] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom\nHenighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma,\nNelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal\nNdousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam\nMcCandlish, Chris Olah, and Jared Kaplan. 2021. A General Language Assistant\nas a Laboratory for Alignment. CoRR abs/2112.00861 (2021).\n[4] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel,\nNihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry,\nZaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David,\nCanwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed\nAlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. Prompt-\nSource: An Integrated Development Environment and Repository for Natural\nLanguage Prompts. In ACL (demo). 93–104.\n[5] Apache Beam. 2023. https://beam.apache.org/\n[6] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,\nKyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and\nOskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models\nAcross Training and Scaling. In ICML, Vol. 202. 2397–2430.\n[7] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence\nGolding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,\nBen Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source\nAutoregressive Language Model. CoRR abs/2204.06745 (2022).\n[8] Andrei Z Broder, Moses Charikar, Alan M Frieze, and Michael Mitzenmacher.\n2000. Min-Wise Independent Permutations. J. Comput. System Sci. 60, 3 (2000),\n630–659.\n[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. In NeurIPS.\n[10] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,\nEric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lund-\nberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023.\nSparks of Artificial General Intelligence: Early experiments with GPT-4. CoRR\nabs/2303.12712 (2023).\n[11] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023.\nLarge Language Models as Tool Makers. CoRR abs/2305.17126 (2023).\n[12] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\nand Kostas Tzoumas. 2015. Apache Flink: Stream and batch processing in a\nsingle engine. IEEE Data Eng. Bull. 38, 4 (2015).\n[13] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian\nTramèr, and Chiyuan Zhang. 2023. Quantifying Memorization Across Neural\nLanguage Models. In ICLR.\n[14] Moses S. Charikar. 2002. Similarity Estimation Techniques from Rounding\nAlgorithms. In STOC. 380–388.\n[15] ChatGLM2-6B . 2023. https://github.com/THUDM/ChatGLM2-6B\n[16] ChatLLaMA. 2023. https://github.com/nebuly-ai/nebuly/tree/main/\noptimization/chatllama\n[17] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav,\nZheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023.\nAlpaGasus: Training A Better Alpaca with Fewer Data. CoRR abs/2307.08701\n(2023).\n[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de\nOliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens\nWinter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,\nAlex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan\nLeike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew\nKnight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,\nDario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating Large Language Models Trained on Code. CoRR abs/2107.03374\n(2021).\n[19] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing\nGPT-4 with 90%* ChatGPT Quality. https://vicuna.lmsys.org\n[20] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fe-\ndus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha\nChowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yan-\nping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.\nScaling Instruction-Finetuned Language Models. CoRR abs/2210.11416 (2022).\n[21] Alibaba Cloud. 2023. https://tongyi.aliyun.com\n[22] Alibaba Cloud. 2023. https://www.alibabacloud.com/en/product/machine-\nlearning\n[23] Yann Collet and Murray Kucherawy. 2021. Zstandard Compression and the\n’application/zstd’ Media Type. RFC 8878.\n[24] Together Computer. 2023. RedPajama: An Open Source Recipe to Reproduce\nLLaMA training dataset . https://github.com/togethercomputer/RedPajama-\nData\n[25] Michael J Cormier, Jonathan R Belyeu, Brent S Pedersen, Joseph Brown, Jo-\nhannes Köster, and Aaron R Quinlan. 2021. Go Get Data (GGD) is a framework\nthat facilitates reproducible access to genomic data. Nature Communications 12,\n1 (2021), 2151.\n[26] Common Crawl. 2023. https://commoncrawl.org/\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn NAACL-HLT (1). 4171–4186.\n[28] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin,\nYuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret\nZoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma\nWang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern,\nToju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen,\nand Claire Cui. 2022. GLaM: Efficient Scaling of Language Models with Mixture-\nof-Experts. In ICML. 5547–5569.\n[29] EleutherAI. 2023. Pythia-1.4B. https://huggingface.co/EleutherAI/pythia-1.4b\n[30] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang.\n2023. Towards Revealing the Mystery behind Chain of Thought: a Theoretical\nPerspective. CoRR abs/2305.15408 (2023).\n[31] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,\nand Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for\nLanguage Modeling. CoRR abs/2101.00027 (2021).\n[32] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,\nand Andy Zou. 2021. A framework for few-shot language model evaluation .\n[33] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A.\nSmith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in\nLanguage Models. In EMNLP (Findings) . 3356–3369.\n[34] Xinyang Geng and Hao Liu. 2023.OpenLLaMA: An Open Reproduction of LLaMA .\nhttps://github.com/openlm-research/open_llama\n[35] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\nTextbooks Are All You Need. arXiv:2306.11644 [cs.CL]\n[36] Project Gutenberg. 2023. https://www.gutenberg.org/\n[37] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong\nQiu, Yuan Yao, Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin,\nYanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie\nTang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. 2021. Pre-\ntrained models: Past, present and future. AI Open 2 (2021), 225–250.\n[38] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. ToolkenGPT:\nAugmenting Frozen Language Models with Massive Tools via Tool Embeddings.\nCoRR abs/2305.11554 (2023).\n[39] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language\nUnderstanding. In ICLR.\n[40] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural\nInstructions: Tuning Language Models with (Almost) No Human Labor. CoRR\nabs/2212.09689 (2022).\n[41] Technology Innovation Institute. 2023. Falcon-RW-1B. https://huggingface.co/\ntiiuae/falcon-rw-1b\n[42] Technology Innovation Institute. 2023. Falcon-RW-1B. https://huggingface.co/\nFlagAlpha/Atom-7B",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 13
      }
    },
    {
      "content": "[43] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,\nTimo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot Learning with Retrieval Augmented Language Models.\nCoRR abs/2208.03299 (2022).\n[44] Abhinav Jain, Hima Patel, Lokesh Nagalapatti, Nitin Gupta, Sameep Mehta,\nShanmukha Guttula, Shashank Mujumdar, Shazia Afzal, Ruhi Sharma Mittal,\nand Vitobha Munigala. 2020. Overview and importance of data quality for\nmachine learning tasks. In KDD. 3561–3562.\n[45] Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and\nXiangang Li. 2023. BELLE: Be Everyone’s Large Language model Engine. https:\n//github.com/LianjiaTech/BELLE.\n[46] jsonargparse. 2023. https://github.com/omni-us/jsonargparse\n[47] Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating Training\nData Mitigates Privacy Risks in Language Models. In ICML. 10697–10707.\n[48] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui\nTam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley,\nRichárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri,\nAndrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick.\n2023. OpenAssistant Conversations - Democratizing Large Language Model\nAlignment. CoRR abs/2304.07327 (2023).\n[49] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for Neural Text Processing. In\nEMNLP.\n[50] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for Neural Text Processing. In\nEMNLP (Demonstration) .\n[51] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Vil-\nlanova del Moral, Teven Le Scao, Leandro von Werra, Chenghao Mou, Ed-\nuardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Sasko, Quentin\nLhoest, Angelina McMillan-Major, Gérard Dupont, Stella Biderman, Anna\nRogers, Loubna Ben Allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, So-\nmaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo\nVillegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber,\nManuel Muñoz, Jian Zhu, Daniel van Strien, Zaid Alyafeai, Khalid Almubarak,\nMinh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pe-\ndro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long\nPhan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana\nIlic, Margaret Mitchell, Alexandra Sasha Luccioni, and Yacine Jernite. 2022.\nThe BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset. In\nNeurIPS.\n[52] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas\nEck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training\nData Makes Language Models Better. In ACL (1). 8424–8445.\n[53] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for\nParameter-Efficient Prompt Tuning. In EMNLP (1) . 3045–3059.\n[54] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In ACL. 7871–7880.\n[55] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur,\nPatrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien\nPlu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya\nMalik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Pa-\ntry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément De-\nlangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault\nGoehringer, Victor Mustar, François Lagunas, Alexander M. Rush, and Thomas\nWolf. 2021. Datasets: A Community Library for Natural Language Processing.\nIn EMNLP (Demos) . 175–184.\n[56] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet\nTalwalkar. 2017. Hyperband: A novel bandit-based approach to hyperparameter\noptimization. J. Mach. Learn. Res. 18 (2017), 185:1–185:52.\n[57] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Ko-\ncetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier De-\nhaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,\nNicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar\nUmapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov,\nMarco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,\nWenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu,\nJennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Car-\nlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von\nWerra, and Harm de Vries. 2023. StarCoder: may the source be with you! CoRR\nabs/2305.06161 (2023).\n[58] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. 2023. ChatDoc-\ntor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain\nKnowledge. CoRR abs/2303.14070 (2023).\n[59] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\nBenjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,\nChristopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson,\nEric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,\nMirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaud-\nhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda.\n2022. Holistic Evaluation of Language Models. CoRR abs/2211.09110 (2022).\n[60] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\nZhu. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.\nCoRR abs/2303.16634 (2023).\n[61] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,\nDenny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The\nFlan Collection: Designing Data and Methods for Effective Instruction Tuning.\nCoRR abs/2301.13688 (2023).\n[62] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts,\nBarret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and\nDaphne Ippolito. 2023. A Pretrainer’s Guide to Training Data: Measuring the\nEffects of Data Age, Domain Coverage, Quality, & Toxicity.CoRR abs/2305.13169\n(2023).\n[63] Ilya Loshchilov and Frank Hutter. 2017. Fixing Weight Decay Regularization in\nAdam. CoRR abs/1711.05101 (2017).\n[64] LZ4. 2023. https://www.lz4.org/\n[65] Kamil Malinka, Martin Peresíni, Anton Firc, Ondrej Hujnak, and Filip Janus.\n2023. On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to\nObtain a University Degree? CoRR abs/2303.11146 (2023).\n[66] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard\nLiaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jor-\ndan, and Ion Stoica. 2018. Ray: A Distributed Framework for Emerging AI\nApplications. In OSDI. 561–577.\n[67] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis. In ICLR.\n[68] OpenAI. 2022. Our approach to alignment research. OpenAI Blog (August\n2022).\n[69] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).\n[70] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instructions with human feedback.\nIn NeurIPS.\n[71] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperform-\ning Curated Corpora with Web Data, and Web Data Only.CoRR abs/2306.01116\n(2023).\n[72] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word\nRepresentations. In NAACL-HLT. 2227–2237.\n[73] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng,\nChuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with Language\nModel Prompting: A Survey. arXiv:2212.09597 [cs.CL]\n[74] Zheng Lin Qingyi Si. 2023. Alpaca-CoT: An Instruction Fine-Tuning Platform\nwith Instruction Data Collection and Unified Large Language Models Interface.\nhttps://github.com/PhoebusSi/alpaca-CoT\n[75] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.\nImproving language understanding by generative pre-training. (2018).\n[76] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. Language models are unsupervised multitask learners.\nOpenAI blog 1, 8 (2019), 9.\n[77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer.J. Mach.\nLearn. Res. (2020), 140:1–140:67.\n[78] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.\nDeepspeed: System optimizations enable training deep learning models with\nover 100 billion parameters. In KDD. 3505–3506.\n[79] Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao\nWang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, An-\ndrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu,\nand Jun Yao. 2023. PanGu-Σ: Towards Trillion Parameter Language Model with",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 14
      }
    },
    {
      "content": "Sparse Heterogeneous Computing. CoRR abs/2303.10845 (2023).\n[80] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias\nGallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson,\nPawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muen-\nnighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier,\nSamson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jer-\nnite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi,\nAitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav,\nCanwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176B-\nParameter Open-Access Multilingual Language Model. CoRR abs/2211.05100\n(2022).\n[81] Omid Shahmirzadi, Adam Lugowski, and Kenneth Younge. 2019. Text similarity\nin vector space models: a comparative study. In ICMLA. 659–666.\n[82] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Fre-\nitas. 2015. Taking the human out of the loop: A review of Bayesian optimization.\nProc. IEEE 104, 1 (2015), 148–175.\n[83] Noam Shazeer. 2020. GLU Variants Improve Transformer. abs/2002.05202\n(2020).\n[84] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting\nZhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580 (2023).\n[85] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\nCasper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion\nParameter Language Models Using Model Parallelism. CoRR abs/1909.08053\n(2019).\n[86] Soldaini, Luca and Lo, Kyle and Kinney, Rodney and Naik, Aakanksha and\nRavichander, Abhilasha and Bhagia, Akshita and Groeneveld, Dirk and Schwenk,\nDustin and Magnusson, Ian and Chandu, Khyathi. 2023. The Dolma Toolkit .\nApache 2.0 License, Version 0.9.0, https://github.com/allenai/dolma.\n[87] Streamlit. 2023. https://streamlit.io/\n[88] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer:\nEnhanced Transformer with Rotary Position Embedding. CoRR abs/2104.09864\n(2021).\n[89] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023.\nPandaGPT: One Model To Instruction-Follow Them All. CoRR abs/2305.16355\n(2023).\n[90] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang,\nJiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,\nWeibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan\nOuyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE 3.0:\nLarge-scale Knowledge Enhanced Pre-training for Language Understanding\nand Generation. CoRR abs/2107.02137 (2021).\n[91] Zhongxiang Sun. 2023. A Short Survey of Viewing Large Language Models in\nLegal Aspect. CoRR abs/2303.09136 (2023).\n[92] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\nalpaca.\n[93] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\nabs/2302.13971 (2023).\n[94] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won\nChung, Iz Beltagy, Julien Launay, and Colin Raffel. 2022. What Language\nModel Architecture and Pretraining Objective Works Best for Zero-Shot Gener-\nalization?. In International Conference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research,\nVol. 162). 22964–22984.\n[95] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amir-\nreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana\nArunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary\nLai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,\nKuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali\nPurohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh\nPuri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,\nSujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-\nNaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP\nTasks. In EMNLP. 5085–5109.\n[96] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language\nModels are Zero-Shot Learners. In ICLR.\n[97] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fe-\ndus. 2022. Emergent Abilities of Large Language Models. CoRR abs/2206.07682\n(2022).\n[98] Jerry W. Wei, Le Hou, Andrew K. Lampinen, Xiangning Chen, Da Huang, Yi Tay,\nXinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. 2023. Symbol\ntuning improves in-context learning in language models. CoRR abs/2305.08298\n(2023).\n[99] Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang,\nPengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan\nHan. 2023. Zero-Shot Information Extraction via Chatting with ChatGPT.CoRR\nabs/2302.10205 (2023).\n[100] Wikipedia. 2023. https://en.wikipedia.org/wiki/Main_Page\n[101] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In EMNLP (Demos) . 38–45.\n[102] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian\nGehrmann, Prabhanjan Kambadur, David S. Rosenberg, and Gideon Mann. 2023.\nBloombergGPT: A Large Language Model for Finance. CoRR abs/2303.17564\n(2023).\n[103] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng\nLu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu. 2023. DoReMi:\nOptimizing Data Mixtures Speeds Up Language Model Pretraining. CoRR\nabs/2305.10429 (2023).\n[104] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. FinGPT: Open-\nSource Financial Large Language Models. CoRR abs/2306.06031 (2023).\n[105] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie\nTang. 2022. GLM-130B: An Open Bilingual Pre-trained Model. abs/2210.02414\n(2022).\n[106] Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization.\nIn NeurIPS. 12360–12371.\n[107] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuo-\nhui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor\nMihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh\nKoura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open\nPre-trained Transformer Language Models. CoRR abs/2205.01068 (2022).\n[108] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,\nYushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large\nLanguage Models. CoRR abs/2303.18223 (2023).\n[109] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang,\nAmin Saied, Weizhu Chen, and Nan Duan. 2023. AGIEval: A Human-Centric\nBenchmark for Evaluating Foundation Models. CoRR abs/2304.06364 (2023).\n[110] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Ur-\ntasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies:\nTowards Story-Like Visual Explanations by Watching Movies and Reading\nBooks. In ICCV. 19–27.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 15
      }
    },
    {
      "content": "APPENDIX OF DATA-JUICER: A ONE-STOP DATA\nPROCESSING SYSTEM FOR LARGE LANGUAGE\nMODELS\nA ADDITIONAL DETAILS OF DATA-JUICER\nA.1 Base Classes of OPs in Data-Juicer\nWe illustrate the core base classes of operators (OPs) inData-Juicer\nat listing 1.\nA.2 Theoretical Analysis of Space Usage for\nCaches and Checkpoints\nCaches are generated after some of the functions of Dataset, such\nas map, filter. Generally, caches can be categorized into cache data\nand indices. The total size of a set of indices is very small so we\ncan ignore these parts when conducting the space usage analysis.\nOn the contrary, the size of the cache data is nearly the same as\nthe input dataset. Here we assume that the sizes of cache data and\ncheckpoints are all the same as the input dataset’s size. And there\nmust be one cache data file for the original dataset after it’s loaded.\nAssume that there are𝑀Mappers, 𝐹Filters, and 𝐷Deduplicators\nin the processing configuration, and the size of the original dataset\nis 𝑆, the detailed analysis for cache mode and checkpoint mode is\nshown below.\nSpace Usage of Cache Mode. Caches are generated after each\nOP. Mappers, Filters, and Deduplicators only generate one set of\ncache data. Besides, the first Filter would generate an extra set of\ncache data because a new column for storing statistics will be added\nto the dataset. Therefore the total disk space usage of caches is:\n𝑆𝑝𝑎𝑐𝑒[𝑐𝑎𝑐ℎ𝑒_𝑚𝑜𝑑𝑒]= (1 +𝑀+𝐹 +I(𝐹 > 0)+𝐷)×𝑆,\nwhere I(·)is the indicator function, which returns 1 when ·is true,\notherwise returns 0.\nSpace Usage of Checkpoint Mode. Checkpoints are only gen-\nerated when any exception or error occurs. However, caches are\nstill stored after disabling the cache mode due to the features of\nDataset. We clean up older caches after each OP. The detailed\ncleanup pipeline is: 1). OP𝑖 finished, 2). caches for OP𝑖 generated,\n3). caches for OP𝑖−1 cleaned up. Thus there exists at most two sets\nof caches at the same time theoretically in step 2. Considering the\ncaches of the original dataset, the peak disk space usage of caches\nin checkpoint mode is:\n𝑆𝑝𝑎𝑐𝑒[𝑐ℎ𝑒𝑐𝑘𝑝𝑜𝑖𝑛𝑡 _𝑚𝑜𝑑𝑒]= 3 ×𝑆.\nB ADDITIONAL NUMERICAL RESULTS\nTable 5: Evaluation results of three types of quality classifiers.\nQuality\nClassifier\nPrecision Recall F1\nGPT-3 96.82% 98.14% 97.47%\nChinese 98.00% 99.30% 98.64%\nCode 71.23% 54.21% 61.56%\n1 class Formatter:\n2 ...\n3 def load_dataset(self, *args) -> Dataset:\n4 ...\n5 ...\n6\n7 class Mapper:\n8 ...\n9 def process(self, sample: Dict) -> Dict:\n10 ...\n11 ...\n12\n13 class Filter:\n14 ...\n15 def compute_stats(self, sample: Dict) -> Dict:\n16 ...\n17\n18 def process(self, sample: Dict) -> bool:\n19 ...\n20 ...\n21\n22 class Deduplicator:\n23 ...\n24 def compute_hash(self, sample: Dict) -> Dict:\n25 ...\n26\n27 def process(self, dataset: Dataset) -> Dataset:\n28 ...\n29 ...\nListing 1: The illustration of OP base classes in Data-Juicer.\nB.1 Quality Classifier\nFirstly, we will show how we can reproduce the GPT-3 and achieve\ncomparable performance.\nWe follow the training procedure of quality classifier in GPT-\n3 [9] that used a logistic regression classifier with features from\nstandard tokenizer and HashingTF of PySpark. Based on this, we\nexpand this training pipeline to Chinese text and various code types.\nThe training details are listed in Table 6, where the keeping method\nincludes:\n• label: 𝑑𝑜𝑐_𝑠𝑐𝑜𝑟𝑒 > 0.5\n• pareto [9]: 𝑑𝑜𝑐_𝑠𝑐𝑜𝑟𝑒 > 1 −𝑛𝑝.𝑟𝑎𝑛𝑑𝑜𝑚.𝑝𝑎𝑟𝑒𝑡𝑜 (𝛼), 𝛼= 9\nWe split these datasets into training and evaluation splits with\na split ratio of 4:1. Then these classifiers trained on the training\nsplit are evaluated on the evaluation split. Experimental results are\nshown in Table 5. As we can see, reproduced GPT-3 and its Chinese\nversion perform well except for the Code version. We speculate\nthat the positive and negative splitting method for Code quality\nclassifier now might not be a good choice, and we leave this issue\nto future research.\nBesides, we compare keeping ratios when using these classifiers\nto re-sample CommonCrawl between the original GPT-3 quality\nclassifier and our reproduced classifiers, which is shown in Table 4.\nThe keeping ratio of the original GPT-3 quality classifier is estimated\nby the data size before and after filtering described in GPT-3 paper\n[9]. We can see that the keeping ratios of our reproduced GPT-3\nquality classifiers are basically aligned with the original one.\nB.2 Data Recipes\nFor pre-training data, we acquired a vast amount of raw textual\ncorpora primarily following the procedural guidelines of RedPa-\njama [24] and the Pile [31]. The common subsets were merged and",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 16
      }
    },
    {
      "content": "Table 6: Training configuration of 3 types of quality classifiers.\nQuality\nClassifier Tokenizer Keep\nMethod Positive Datasets Negative Datasets\nGPT-3 Standard\nTokenizer pareto Wikipedia-en & books1 &\nOpenWebText2 CommonCrawl\nChinese Sentencepiece label Wikipeida-zh & Wudao Samples in Chinese from CommonCrawl\nCode Sentencepiece label Samples with max_stars_count>=1372\nfrom TheStack\nRandom Samples from the rest of\nTheStack\nsubjected to Data-Juicer refinements. The resultant data recipe is\npresented in Table 7, which covers 15 prominent components. We\nuse the SentencePiece [50] tokenizer as implemented in GPT-NeoX-\n20B [7] to prepare text and report the counted number of tokens.\nThe sampling proportion is the normalization of token numbers,\nexcept for Books and Wikipedia, which undergo 2 and 2.5 epochs\nrespectively, to enhance the weighting of high-quality corpora.\nTable 7: Statistics of Data-Juicer’s pre-training data.\nComponent #Tokens Sampling prop.\nCommonCrawl 360,925,581,674 44 .91%\nC4 181,951,688,729 22 .64%\nGitHub 65,076,921,292 8 .10%\nBooks 26,389,944,579 6 .57%\nWikipedia 17,615,935,449 5 .48%\narXiv 29,093,082,586 3 .62%\nPubMed Central 25,589,708,647 3 .18%\nStackExchange 19,793,629,900 2 .46%\nFreeLaw 13,057,506,102 1 .62%\nPubMed Abstracts 5,208,343,613 0 .65%\nUSPTO 4,021,281,155 0 .50%\nEuroParl 780,962,770 0 .10%\nHackerNews 485,584,871 0 .06%\nPhilPapers 478,040,431 0 .06%\nHIH ExPorter 436,414,852 0 .05%\nFor fine-tuning data, we merge and refine tens of Alpaca-CoT\ndatasets. Each dataset can be categorized into English, Chinese\nand Multilingual by language; into instruct fine-tuning, and chat\nfine-tuning including sinlge-round dialog, multi-round dialog and\npreference by usage; multi-task and task-specific by task type; and\nhuman-generated, self-instruct, and mixed collection of datasets by\nthe generation method. The detailed numbers of datasets for each\ncategory are presented in Table 8.\nMore information about these datasets can be found on the\nData-Juicer recipes page2 of our repository.\n2https://github.com/alibaba/data-juicer/blob/main/configs/data_juicer_recipes\nTable 8: Statistics of Data-Juicer fine-tuning data used in our\nexperiments. ∗These tags are newly added by Data-Juicer\ncompared to the original tag sets of Alpaca-CoT [ 74].“CFT”\nindicates Chat Fine-Tuning.\nCategory Sub-Category #Datasets\nLanguage\nEnglish 28\nChinese 14\nMultilingual 3\nUsage∗\nInstruct Fine-Tuning (IFT) 17\nCFT: Single-Round Dialog 23\nCFT: Multi-Round Dialog 2\nCFT: Preference 5\nTask Type\nMulti-Task 27\nTask-Specific 13\nGeneration Method\nHuman-Generated 3\nSelf-Instruct 12\nMixted 5\nCollection of Datasets 19\nB.3 Experiments Details\nB.3.1 Models and Training For Pre-training Data. We adhere\nto the official paper [93] and leverage open-source implementation\n[34] to build standard LLaMA models. Basically, it is to apply RM-\nSNorm [106], the SwiGLU activation [ 83], and rotary positional\nembedding [88] on the decoder-only transformer architecture. The\nLLaMA-1.3B model is composed of 24 transformer layers, each with\n16 self-attention heads and 2048 bottleneck units.\nLLMs are pre-trained using the AdamW optimizer [ 63] with\nhyper-parameters 𝛽1 = 0.9 and 𝛽2 = 0.95. For LLaMA-1.3B, the\ninitial learning rate gradually increases to 2e-5 using 1% warm-up\nsteps and finally decays to 10% through a cosine schedule. The\nweight decay is set to 0.1 and the gradient ℓ2-norm is clipped to 1.0.\nB.3.2 Models and Training of Fine-Tuning Data.In fine-tuning,\nwe choose LLaMA-7B as our basic model and fine-tuned it for 3\nepochs. We follow the hyper-parameter settings in Alpaca [ 92].\nSpecifically, the optimizer is AdamW with a learning rate of 2e-5,",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 17
      }
    },
    {
      "content": "global batch size of 256, and weight decay of 0. The learning rate\nschedules in a cosine style with 3% initial warm-up steps.\nRegarding the data recipes in Table 3, for (CFT, EN) case, we\nconsider 5 competitive subsets (Alpaca, GPTeacher, FastChat, Gua-\nnaco, and CodeAlpaca) from Alpaca-CoT as candidate datasets; for\n(CFT, ZH) case, we use (AlpacaGPT4, Belle, Instinwild) as candi-\ndate datasets. Generally speaking, we bucket from these candidate\ndatasets according to more than a dozen built-in analytical dimen-\nsions, sampling a fixed amount of data from each dimension to\nincrease the diversity of the processed data as appropriately as\npossible. More detailed hyper-parameters of data processing can\nbe found in our released data recipes.\nBoth the pre-trained and fine-tuned reference models are re-\nleased in our homepage.\nB.3.3 System Performance Experiments. The experiments of\nend-to-end processing mentioned in section 7.2.1 are all conducted\non the same machine with 128 cores of Intel(R) Xeon(R) Platinum\n8369B models and about 990GB memory. Before starting these\nexperiments, the original datasets, third-party models, and other as-\nsets will be prepared in advance for both baselines andData-Juicer,\nand the intermediate cache files will be cleaned after every com-\nplete process for Data-Juicer. After processing, we use the same\nnumber of processes for processing the dataset to export the result\ndataset to the local SSD.\nAs for the resource monitoring tool, it’s implemented based on\nthe psutil3 library. It samples the memory for all related processes\nevery second during the processing pipeline. Then we compute the\naverage memory usage by summing the memory usage over all\nprocesses and dividing by the number of processes used in each\nexperiment. Finally, we aggregate all data and compute the average\nmemory usage over time.\nB.3.4 End-to-end System Baselines. We mainly compared the\nend-to-end system performance between our Data-Juicer and\ntwo state-of-the-art baselines in the above experiments w.r.t system\nperformance: RedPajama [24] and Dolma [86]. Besides the empirical\ncomparsiton in Sec.7.2.1, here we make more detailed introduction\nand comparison about them.\nRedPajama. 4 The RedPajama project, developed by Together\nAI, initially aims to reproduce the LLaMA training dataset [ 93]\nand open-source the entire code for data collection and processing,\nmaking it a significant and popular contribution to the LLM com-\nmunity. This is the primary reason for selecting it as our baseline.\nRedPajama provides a reproduced version of all seven subsets of\nthe LLaMA training dataset, including arXiv, Books, C4, Common-\nCrawl, GitHub Code, Stack Exchange, and Wikipedia.\nWhile RedPajama has made valuable contributions, our work\nexplores different aspects and offers complementary features. For\ninstance: (1) RedPajama’s design is closely tied to specific datasets,\nwhich present challenges for adapting its data processing pipelines\nto other datasets. (2) Its focus on reproducing the LLaMA datasets\nlead to trade-offs in efficiency, which is not the primary concern of\nthe RedPajama project. (3) The current data processing component\nin RedPajama lacks systematization and customization. Adding new\n3https://github.com/giampaolo/psutil\n4We compared RedPajama in our experiments with its github commit ID as:\n45b37c2a1d1e495b0f48549ef3ce03ff029f7881.\ndata processing methods to the existing pipelines would require\nunderstanding and modifying a significant portion of the code. As\na result, most users typically opt to utilize the RedPajama Dataset\ndirectly rather than attempting to customize or improve its data\nprocessing pipelines.\nDolma. 5 The Dolma project, originating from Allen AI , com-\nprises two components: the Dolma Dataset and the Dolma Toolkit.\nIt is also a newly established data processing initiative. We selected\nthe Dolma Toolkit as a baseline because its objective of generating\npre-training data for language modeling aligns with one of our\ntarget data types (we focus on both pre-training and fine-tuning\ndata). The toolkit offers numerous “Taggers” that enable attribute\ntagging (analogous to ’stats’ in Data-Juicer) for each document\nsample. These tags are then used to filter out samples with undesir-\nable attributes. Users have the flexibility to create custom taggers\ntailored to their specific needs.\nHowever, we encountered several limitations when using Dolma\nfor dataset processing. Firstly, Dolma’s workflow involves multi-\nple stages—tagging, deduplication, mixing, and various configura-\ntions—lacking support for an end-to-end data processing pipeline.\nSecondly, to leverage high-performance parallel processing, users\nare required to partition the input dataset into multiple shards in\nadvance, incurring additional overhead. Thirdly, Dolma imposes\ncertain requirements on input datasets, such as mandatory fields\nand a specific directory structure, necessitating further preprocess-\ning before use. Moreover, it restricts input formats to JSONL or its\ngzipped variant. These constraints diminish the toolkit’s flexibility,\nthereby increasing the cost of use and rendering the Dolma Toolkit\nrelatively less user-friendly.\nB.3.5 Scalability. Our experiments are performed on a platform\ncomprising 16 servers, each equipped with a 64-core Intel(R) Xeon(R)\nPlatinum CPU (mix of 8269CY and 8163 models) and 512 GB of\nmemory. The network bandwidth shared among these servers is 20\nGbps. We utilize NAS storage to house both the raw data and the\nprocessed results. For the scalability experiments, we consider the\ntwo baselines as follows:\n•Data-Juicer on Ray : We implement a Ray [66] executor for\nData-Juicer, which only adaptes the underlying interfaces of\nthe HuggingFace-datasets with Ray-datasets, while all OPs of\nData-Juicer remain unchanged. This implies that users’ code\nbased on our native Python version can be seamlessly migrated\nfrom a single-machine version to distributed computing environ-\nments.\n•Data-Juicer on Beam : This method is based on Apache Beam\nwith the Apache Flink Runner. When compared to the Ray ver-\nsion, the Beam version requires additional code development to\nmeet the demands of the Beam data processing pipeline. This in-\ncludes the adaptations of several OPs and the replacement of the\nFormatter/Exporter with counterparts in Beam.\nB.4 Per-Task Evaluation\nFor a thorough and consolidated assessment, we have summarized\nthe individual scores of evaluated LLMs on the 16 core HELM\nassessment tasks in Table 9.\n5We compared Dolma in our experiments with its github commit ID as:\n5a010a2685914b1db7744426abfb4b9ece52da95.",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 18
      }
    },
    {
      "content": "Table 9: Evaluation results on 16 core tasks of HELM benchmark.\nTask Falcon-1.3B Pythia-1.4B LLaMA-1.3B\n(Data-Juicer)\nLLaMA-1.3B\n(Data-Juicer IFT)\nMMLU 24.7 26 .0 25 .9 27 .0\nBoolQ 63.0 56 .0 49 .0 56 .0\nNarrativeQA 32.1 31 .5 38 .2 49 .9\nNaturalQuestions (closed-book) 10.7 10 .5 10 .1 11 .2\nNaturalQuestions (open-book) 50.0 49 .8 45 .9 54 .3\nQuAC 24.3 26 .5 26 .0 21 .7\nHellaSwag 67.0 57 .0 56 .0 52 .0\nOpenbookQA 44.0 34 .0 40 .0 43 .0\nTruthfulQA 19.0 21 .0 33 .0 33 .0\nMS MARCO (regular) 16.8 12 .9 11 .2 12 .1\nMS MARCO (TREC) 33.5 27 .4 26 .9 28 .1\nIMDB 55.0 84 .0 80 .0 84 .0\nXSUM 5.7 6 .5 5 .2 5 .3\nCNN/DailyMail 4.0 8 .4 7 .8 11 .1\nCivilComments 49.4 49 .7 50 .1 50 .0\nRAFT 44.3 42 .3 42 .1 49 .3",
      "metadata": {
        "source": "C:\\Users\\19706\\AppData\\Local\\Temp\\tmp38vnlq5c\\222\\2309.02033v3.pdf",
        "page": 19
      }
    }
  ]
}